{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Layer\n",
    "class Layer_LSTM:\n",
    "    def __init__(self, n_inputs, n_neurons, n_outputs):\n",
    "        # Initialize weights for input, forget, cell, and output gates\n",
    "        self.weights_input = 0.01 * np.random.randn(n_inputs, n_neurons * 4)\n",
    "        self.weights_hidden = 0.01 * np.random.randn(n_neurons, n_neurons * 4)\n",
    "        self.biases = np.zeros((1, n_neurons * 4))\n",
    "\n",
    "        # Output layer weights\n",
    "        self.weights_output = 0.01 * np.random.randn(n_neurons, n_outputs)\n",
    "        self.bias_output = np.zeros((1, n_outputs))\n",
    "\n",
    "        # Store hidden and cell states\n",
    "        self.hidden_state = np.zeros((1, n_neurons))\n",
    "        self.cell_state = np.zeros((1, n_neurons))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Compute gate activations\n",
    "        gates = (np.dot(inputs, self.weights_input) +\n",
    "                 np.dot(self.hidden_state, self.weights_hidden) + \n",
    "                 self.biases)\n",
    "\n",
    "        # Split gate outputs\n",
    "        i_gate, f_gate, c_gate, o_gate = np.split(gates, 4, axis=1)\n",
    "\n",
    "        # Apply activations\n",
    "        i_gate = self.sigmoid(i_gate)  # Input gate\n",
    "        f_gate = self.sigmoid(f_gate)  # Forget gate\n",
    "        c_gate = np.tanh(c_gate)       # Candidate cell state\n",
    "        o_gate = self.sigmoid(o_gate)  # Output gate\n",
    "\n",
    "        # Update cell state\n",
    "        self.cell_state = f_gate * self.cell_state + i_gate * c_gate\n",
    "\n",
    "        # Compute hidden state\n",
    "        self.hidden_state = o_gate * np.tanh(self.cell_state)\n",
    "\n",
    "        # Output layer computation\n",
    "        self.output = np.dot(self.hidden_state, self.weights_output) + self.bias_output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients for output layer\n",
    "        self.dweights_output = np.dot(self.hidden_state.T, dvalues)\n",
    "        self.dbias_output = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient for hidden state\n",
    "        d_hidden = np.dot(dvalues, self.weights_output.T)\n",
    "\n",
    "        # Compute gate gradients\n",
    "        d_o_gate = d_hidden * np.tanh(self.cell_state)\n",
    "        d_cell_state = d_hidden * self.sigmoid(d_o_gate) * (1 - np.tanh(self.cell_state) ** 2)\n",
    "\n",
    "        # Compute gradients for input, forget, and candidate gates\n",
    "        d_i_gate = d_cell_state * np.tanh(self.cell_state)\n",
    "        d_f_gate = d_cell_state * self.cell_state\n",
    "        d_c_gate = d_cell_state * self.sigmoid(d_i_gate)\n",
    "\n",
    "        # Apply activations' derivative\n",
    "        d_i_gate *= self.sigmoid(d_i_gate) * (1 - self.sigmoid(d_i_gate))\n",
    "        d_f_gate *= self.sigmoid(d_f_gate) * (1 - self.sigmoid(d_f_gate))\n",
    "        d_o_gate *= self.sigmoid(d_o_gate) * (1 - self.sigmoid(d_o_gate))\n",
    "        d_c_gate *= 1 - np.tanh(d_c_gate) ** 2\n",
    "\n",
    "        # Combine gate gradients\n",
    "        d_gates = np.hstack((d_i_gate, d_f_gate, d_c_gate, d_o_gate))\n",
    "\n",
    "        # Compute input and hidden weight gradients\n",
    "        self.dweights_input = np.dot(self.inputs.T, d_gates)\n",
    "        self.dweights_hidden = np.dot(self.hidden_state.T, d_gates)\n",
    "        self.dbiases = np.sum(d_gates, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute input gradient for previous time step\n",
    "        self.dinputs = np.dot(d_gates, self.weights_input.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation for Output Layer\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function (Cross Entropy for Classification)\n",
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        return -np.log(correct_confidences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, lstm_layer):\n",
    "        if not hasattr(lstm_layer, 'weight_momentums_input'):\n",
    "            # Initialize momentums and caches for input weights\n",
    "            lstm_layer.weight_momentums_input = np.zeros_like(lstm_layer.weights_input)\n",
    "            lstm_layer.weight_cache_input = np.zeros_like(lstm_layer.weights_input)\n",
    "\n",
    "            # Initialize momentums and caches for hidden weights\n",
    "            lstm_layer.weight_momentums_hidden = np.zeros_like(lstm_layer.weights_hidden)\n",
    "            lstm_layer.weight_cache_hidden = np.zeros_like(lstm_layer.weights_hidden)\n",
    "\n",
    "            # Initialize momentums and caches for biases\n",
    "            lstm_layer.bias_momentums = np.zeros_like(lstm_layer.biases)\n",
    "            lstm_layer.bias_cache = np.zeros_like(lstm_layer.biases)\n",
    "\n",
    "        # Compute momentums for input weights\n",
    "        lstm_layer.weight_momentums_input = self.beta_1 * lstm_layer.weight_momentums_input + \\\n",
    "                                            (1 - self.beta_1) * lstm_layer.dweights_input\n",
    "        # Compute momentums for hidden weights\n",
    "        lstm_layer.weight_momentums_hidden = self.beta_1 * lstm_layer.weight_momentums_hidden + \\\n",
    "                                             (1 - self.beta_1) * lstm_layer.dweights_hidden\n",
    "        # Compute momentums for biases\n",
    "        lstm_layer.bias_momentums = self.beta_1 * lstm_layer.bias_momentums + \\\n",
    "                                    (1 - self.beta_1) * lstm_layer.dbiases\n",
    "\n",
    "        # Bias correction for momentums\n",
    "        weight_momentums_input_corrected = lstm_layer.weight_momentums_input / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        weight_momentums_hidden_corrected = lstm_layer.weight_momentums_hidden / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = lstm_layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Compute cache updates for input weights\n",
    "        lstm_layer.weight_cache_input = self.beta_2 * lstm_layer.weight_cache_input + \\\n",
    "                                        (1 - self.beta_2) * lstm_layer.dweights_input ** 2\n",
    "        # Compute cache updates for hidden weights\n",
    "        lstm_layer.weight_cache_hidden = self.beta_2 * lstm_layer.weight_cache_hidden + \\\n",
    "                                         (1 - self.beta_2) * lstm_layer.dweights_hidden ** 2\n",
    "        # Compute cache updates for biases\n",
    "        lstm_layer.bias_cache = self.beta_2 * lstm_layer.bias_cache + \\\n",
    "                                (1 - self.beta_2) * lstm_layer.dbiases ** 2\n",
    "\n",
    "        # Bias correction for cache values\n",
    "        weight_cache_input_corrected = lstm_layer.weight_cache_input / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        weight_cache_hidden_corrected = lstm_layer.weight_cache_hidden / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = lstm_layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Update input weights\n",
    "        lstm_layer.weights_input -= self.current_learning_rate * weight_momentums_input_corrected / \\\n",
    "                                    (np.sqrt(weight_cache_input_corrected) + self.epsilon)\n",
    "        # Update hidden weights\n",
    "        lstm_layer.weights_hidden -= self.current_learning_rate * weight_momentums_hidden_corrected / \\\n",
    "                                     (np.sqrt(weight_cache_hidden_corrected) + self.epsilon)\n",
    "        # Update biases\n",
    "        lstm_layer.biases -= self.current_learning_rate * bias_momentums_corrected / \\\n",
    "                             (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy: 0.6000, Loss: 0.6931481759806717\n",
      "Epoch 100, Accuracy: 0.9000, Loss: 0.6734842418364076\n",
      "Epoch 200, Accuracy: 0.9000, Loss: 0.673607035232079\n",
      "Epoch 300, Accuracy: 0.9000, Loss: 0.6736678073772002\n",
      "Epoch 400, Accuracy: 0.9000, Loss: 0.673704627841007\n",
      "Epoch 500, Accuracy: 0.9000, Loss: 0.6737290448039464\n",
      "Epoch 600, Accuracy: 0.9000, Loss: 0.6737459934245937\n",
      "Epoch 700, Accuracy: 0.9000, Loss: 0.67375804262496\n",
      "Epoch 800, Accuracy: 0.9000, Loss: 0.673766711951876\n",
      "Epoch 900, Accuracy: 0.9000, Loss: 0.6737729823732204\n",
      "Predicted Class: [0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM Layer\n",
    "lstm_layer = Layer_LSTM(n_inputs=3, n_neurons=5, n_outputs=2)\n",
    "\n",
    "# Activation & Loss\n",
    "activation_softmax = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.01)\n",
    "\n",
    "# Dummy data\n",
    "X = np.random.randn(10, 3)  # 10 samples, 3 features\n",
    "y = np.random.randint(0, 2, size=(10,))  # 10 target labels (0 or 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    lstm_layer.forward(X)\n",
    "    activation_softmax.forward(lstm_layer.output)\n",
    "    loss = loss_function.forward(activation_softmax.output, y)\n",
    "\n",
    "    # Get predictions (class with highest probability)\n",
    "    predictions = np.argmax(activation_softmax.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # Backward pass\n",
    "    lstm_layer.backward(activation_softmax.output - np.eye(2)[y])\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.update_params(lstm_layer)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Accuracy: {accuracy:.4f}, Loss: {loss.mean()}\")\n",
    "\n",
    "# Test on a new sample\n",
    "X_test = np.array([[0.7, 0.7, -0.2]])\n",
    "lstm_layer.forward(X_test)\n",
    "activation_softmax.forward(lstm_layer.output)\n",
    "predictions = np.argmax(activation_softmax.output, axis=1)\n",
    "\n",
    "print(\"Predicted Class:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
