{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_RNN:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, n_outputs):\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.weights_hidden = 0.01 * np.random.randn(n_neurons, n_neurons)\n",
    "        self.weights_output = 0.01 * np.random.randn(n_neurons, n_outputs)\n",
    "\n",
    "        # Bias vectors\n",
    "        self.bias_hidden = np.zeros((1, n_neurons))\n",
    "        self.bias_output = np.zeros((1, n_outputs))\n",
    "\n",
    "        # Store hidden state\n",
    "        self.hidden_state = np.zeros((1, n_neurons))\n",
    "\n",
    "       \n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.hidden_state = np.tanh(\n",
    "            np.dot(inputs, self.weights_input) +\n",
    "            np.dot(self.hidden_state, self.weights_hidden) +\n",
    "            self.bias_hidden\n",
    "        )\n",
    "        self.output = np.dot(self.hidden_state, self.weights_output) + self.bias_output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient for output layer\n",
    "        self.dweights_output = np.dot(self.hidden_state.T, dvalues)\n",
    "        self.dbias_output = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient for hidden layer\n",
    "        dhidden = np.dot(dvalues, self.weights_output.T) * (1 - self.hidden_state ** 2)\n",
    "        self.dweights_input = np.dot(self.inputs.T, dhidden)\n",
    "        self.dweights_hidden = np.dot(self.hidden_state.T, dhidden)\n",
    "        self.dbias_hidden = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient for next time step\n",
    "        self.dinputs = np.dot(dhidden, self.weights_input.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh for RNN hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Tanh:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        return -np.log(correct_confidences)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs /= samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.forward(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs /= samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights_input)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights_input)\n",
    "            layer.bias_momentums = np.zeros_like(layer.bias_hidden)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias_hidden)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights_input\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbias_hidden\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights_input ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbias_hidden ** 2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights_input -= self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.bias_hidden -= self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, accuracy: 0.1000, Loss: 0.6932619461532649\n",
      "Epoch:1, accuracy: 0.5000, Loss: 0.6930317637548876\n",
      "Epoch:2, accuracy: 0.6000, Loss: 0.6928021588520837\n",
      "Epoch:3, accuracy: 0.7000, Loss: 0.6925736288067998\n",
      "Epoch:4, accuracy: 0.7000, Loss: 0.6923464080641659\n",
      "Epoch:5, accuracy: 0.7000, Loss: 0.6921207991532565\n",
      "Epoch:6, accuracy: 0.7000, Loss: 0.6918971260156808\n",
      "Epoch:7, accuracy: 0.7000, Loss: 0.6916756916810521\n",
      "Epoch:8, accuracy: 0.7000, Loss: 0.6914567799975709\n",
      "Epoch:9, accuracy: 0.7000, Loss: 0.6912406396393361\n",
      "Epoch:10, accuracy: 0.7000, Loss: 0.6910274483096227\n",
      "Epoch:11, accuracy: 0.7000, Loss: 0.6908173069142809\n",
      "Epoch:12, accuracy: 0.7000, Loss: 0.6906102809059232\n",
      "Epoch:13, accuracy: 0.7000, Loss: 0.6904063529682511\n",
      "Epoch:14, accuracy: 0.7000, Loss: 0.6902053909294052\n",
      "Epoch:15, accuracy: 0.7000, Loss: 0.6900072050889414\n",
      "Epoch:16, accuracy: 0.7000, Loss: 0.6898115822240688\n",
      "Epoch:17, accuracy: 0.7000, Loss: 0.6896182967894782\n",
      "Epoch:18, accuracy: 0.7000, Loss: 0.689427133378737\n",
      "Epoch:19, accuracy: 0.7000, Loss: 0.689237912308292\n",
      "Epoch:20, accuracy: 0.7000, Loss: 0.6890505426793703\n",
      "Epoch:21, accuracy: 0.7000, Loss: 0.6888650040504256\n",
      "Epoch:22, accuracy: 0.7000, Loss: 0.6886813136481225\n",
      "Epoch:23, accuracy: 0.7000, Loss: 0.6884995535781191\n",
      "Epoch:24, accuracy: 0.7000, Loss: 0.6883198619554827\n",
      "Epoch:25, accuracy: 0.7000, Loss: 0.6881423840262202\n",
      "Epoch:26, accuracy: 0.7000, Loss: 0.6879672725693472\n",
      "Epoch:27, accuracy: 0.8000, Loss: 0.6877946946029503\n",
      "Epoch:28, accuracy: 0.8000, Loss: 0.6876248073027464\n",
      "Epoch:29, accuracy: 0.8000, Loss: 0.687457745418106\n",
      "Epoch:30, accuracy: 0.8000, Loss: 0.6872936307004063\n",
      "Epoch:31, accuracy: 0.8000, Loss: 0.6871325708036444\n",
      "Epoch:32, accuracy: 0.8000, Loss: 0.6869746337787619\n",
      "Epoch:33, accuracy: 0.8000, Loss: 0.6868198680782586\n",
      "Epoch:34, accuracy: 0.8000, Loss: 0.6866683033164422\n",
      "Epoch:35, accuracy: 0.8000, Loss: 0.6865199322686658\n",
      "Epoch:36, accuracy: 0.8000, Loss: 0.6863747323001479\n",
      "Epoch:37, accuracy: 0.8000, Loss: 0.686232663627667\n",
      "Epoch:38, accuracy: 0.8000, Loss: 0.6860936667644315\n",
      "Epoch:39, accuracy: 0.8000, Loss: 0.685957672643718\n",
      "Epoch:40, accuracy: 0.8000, Loss: 0.6858246131060632\n",
      "Epoch:41, accuracy: 0.8000, Loss: 0.6856944116554498\n",
      "Epoch:42, accuracy: 0.8000, Loss: 0.6855669983060411\n",
      "Epoch:43, accuracy: 0.8000, Loss: 0.6854423127394501\n",
      "Epoch:44, accuracy: 0.8000, Loss: 0.6853202944903468\n",
      "Epoch:45, accuracy: 0.8000, Loss: 0.6852008965355217\n",
      "Epoch:46, accuracy: 0.8000, Loss: 0.685084077736955\n",
      "Epoch:47, accuracy: 0.8000, Loss: 0.6849698015581762\n",
      "Epoch:48, accuracy: 0.8000, Loss: 0.6848580366016014\n",
      "Epoch:49, accuracy: 0.8000, Loss: 0.6847487526104065\n",
      "Epoch:50, accuracy: 0.8000, Loss: 0.6846419167881199\n",
      "Epoch:51, accuracy: 0.8000, Loss: 0.6845374961939183\n",
      "Epoch:52, accuracy: 0.8000, Loss: 0.6844354496838617\n",
      "Epoch:53, accuracy: 0.8000, Loss: 0.6843357331013773\n",
      "Epoch:54, accuracy: 0.8000, Loss: 0.6842382977348317\n",
      "Epoch:55, accuracy: 0.8000, Loss: 0.6841430883463541\n",
      "Epoch:56, accuracy: 0.8000, Loss: 0.6840500475142641\n",
      "Epoch:57, accuracy: 0.8000, Loss: 0.6839591125709459\n",
      "Epoch:58, accuracy: 0.8000, Loss: 0.6838702193320387\n",
      "Epoch:59, accuracy: 0.8000, Loss: 0.6837833062395844\n",
      "Epoch:60, accuracy: 0.8000, Loss: 0.683698311323437\n",
      "Epoch:61, accuracy: 0.8000, Loss: 0.683615173581926\n",
      "Epoch:62, accuracy: 0.8000, Loss: 0.6835338368495927\n",
      "Epoch:63, accuracy: 0.8000, Loss: 0.6834542457980828\n",
      "Epoch:64, accuracy: 0.8000, Loss: 0.6833763497940792\n",
      "Epoch:65, accuracy: 0.8000, Loss: 0.6833001009388731\n",
      "Epoch:66, accuracy: 0.8000, Loss: 0.6832254531439466\n",
      "Epoch:67, accuracy: 0.8000, Loss: 0.6831523633782051\n",
      "Epoch:68, accuracy: 0.8000, Loss: 0.6830807905426853\n",
      "Epoch:69, accuracy: 0.8000, Loss: 0.683010693288414\n",
      "Epoch:70, accuracy: 0.8000, Loss: 0.6829420319348225\n",
      "Epoch:71, accuracy: 0.8000, Loss: 0.6828747677927901\n",
      "Epoch:72, accuracy: 0.8000, Loss: 0.6828088626884284\n",
      "Epoch:73, accuracy: 0.8000, Loss: 0.6827442777819254\n",
      "Epoch:74, accuracy: 0.8000, Loss: 0.6826809751254672\n",
      "Epoch:75, accuracy: 0.8000, Loss: 0.6826189173734177\n",
      "Epoch:76, accuracy: 0.8000, Loss: 0.6825580679971931\n",
      "Epoch:77, accuracy: 0.8000, Loss: 0.6824983913018169\n",
      "Epoch:78, accuracy: 0.8000, Loss: 0.6824398517903383\n",
      "Epoch:79, accuracy: 0.8000, Loss: 0.6823824152868772\n",
      "Epoch:80, accuracy: 0.8000, Loss: 0.6823260498690699\n",
      "Epoch:81, accuracy: 0.8000, Loss: 0.6822707237743593\n",
      "Epoch:82, accuracy: 0.8000, Loss: 0.6822164064319751\n",
      "Epoch:83, accuracy: 0.8000, Loss: 0.6821630691954116\n",
      "Epoch:84, accuracy: 0.8000, Loss: 0.682110684098051\n",
      "Epoch:85, accuracy: 0.8000, Loss: 0.6820592241216381\n",
      "Epoch:86, accuracy: 0.8000, Loss: 0.6820086634411073\n",
      "Epoch:87, accuracy: 0.8000, Loss: 0.6819589770615984\n",
      "Epoch:88, accuracy: 0.8000, Loss: 0.6819101410673831\n",
      "Epoch:89, accuracy: 0.8000, Loss: 0.6818621320303951\n",
      "Epoch:90, accuracy: 0.8000, Loss: 0.6818149266694284\n",
      "Epoch:91, accuracy: 0.8000, Loss: 0.6817685027938876\n",
      "Epoch:92, accuracy: 0.8000, Loss: 0.6817228394434056\n",
      "Epoch:93, accuracy: 0.8000, Loss: 0.6816779156934272\n",
      "Epoch:94, accuracy: 0.8000, Loss: 0.6816337108601567\n",
      "Epoch:95, accuracy: 0.8000, Loss: 0.6815902053835259\n",
      "Epoch:96, accuracy: 0.8000, Loss: 0.6815473803394182\n",
      "Epoch:97, accuracy: 0.8000, Loss: 0.6815052173174647\n",
      "Epoch:98, accuracy: 0.8000, Loss: 0.6814636986114986\n",
      "Epoch:99, accuracy: 0.8000, Loss: 0.6814228070094378\n",
      "Epoch:100, accuracy: 0.8000, Loss: 0.6813825260369195\n",
      "Epoch:101, accuracy: 0.8000, Loss: 0.6813428399041872\n",
      "Epoch:102, accuracy: 0.8000, Loss: 0.6813037330510088\n",
      "Epoch:103, accuracy: 0.8000, Loss: 0.6812651906496827\n",
      "Epoch:104, accuracy: 0.8000, Loss: 0.6812271987074878\n",
      "Epoch:105, accuracy: 0.8000, Loss: 0.6811897433759615\n",
      "Epoch:106, accuracy: 0.8000, Loss: 0.68115281104433\n",
      "Epoch:107, accuracy: 0.8000, Loss: 0.6811163889251821\n",
      "Epoch:108, accuracy: 0.8000, Loss: 0.6810804647318067\n",
      "Epoch:109, accuracy: 0.8000, Loss: 0.681045026331911\n",
      "Epoch:110, accuracy: 0.8000, Loss: 0.6810100619200925\n",
      "Epoch:111, accuracy: 0.8000, Loss: 0.68097556012129\n",
      "Epoch:112, accuracy: 0.8000, Loss: 0.6809415100420547\n",
      "Epoch:113, accuracy: 0.8000, Loss: 0.6809079012180842\n",
      "Epoch:114, accuracy: 0.8000, Loss: 0.6808747232612609\n",
      "Epoch:115, accuracy: 0.8000, Loss: 0.6808419660627536\n",
      "Epoch:116, accuracy: 0.8000, Loss: 0.6808096201062239\n",
      "Epoch:117, accuracy: 0.8000, Loss: 0.6807776761415512\n",
      "Epoch:118, accuracy: 0.8000, Loss: 0.6807461250182962\n",
      "Epoch:119, accuracy: 0.8000, Loss: 0.6807149580152638\n",
      "Epoch:120, accuracy: 0.9000, Loss: 0.6806841667556013\n",
      "Epoch:121, accuracy: 0.9000, Loss: 0.6806537429733241\n",
      "Epoch:122, accuracy: 0.9000, Loss: 0.6806236786542887\n",
      "Epoch:123, accuracy: 0.9000, Loss: 0.6805939661515389\n",
      "Epoch:124, accuracy: 0.9000, Loss: 0.6805645980590007\n",
      "Epoch:125, accuracy: 0.9000, Loss: 0.6805355671553203\n",
      "Epoch:126, accuracy: 0.9000, Loss: 0.6805068663512728\n",
      "Epoch:127, accuracy: 0.9000, Loss: 0.6804784887708284\n",
      "Epoch:128, accuracy: 0.9000, Loss: 0.6804504279004286\n",
      "Epoch:129, accuracy: 0.9000, Loss: 0.6804226774181334\n",
      "Epoch:130, accuracy: 0.9000, Loss: 0.6803952309994621\n",
      "Epoch:131, accuracy: 0.9000, Loss: 0.6803680825451689\n",
      "Epoch:132, accuracy: 0.9000, Loss: 0.6803412262897239\n",
      "Epoch:133, accuracy: 0.9000, Loss: 0.6803146565817396\n",
      "Epoch:134, accuracy: 0.9000, Loss: 0.6802883678407543\n",
      "Epoch:135, accuracy: 0.9000, Loss: 0.680262354701902\n",
      "Epoch:136, accuracy: 0.9000, Loss: 0.6802366119893635\n",
      "Epoch:137, accuracy: 0.9000, Loss: 0.6802111346567252\n",
      "Epoch:138, accuracy: 0.9000, Loss: 0.6801859177963103\n",
      "Epoch:139, accuracy: 0.9000, Loss: 0.6801609566380545\n",
      "Epoch:140, accuracy: 0.9000, Loss: 0.6801362465630125\n",
      "Epoch:141, accuracy: 0.9000, Loss: 0.680111783093819\n",
      "Epoch:142, accuracy: 0.9000, Loss: 0.6800875618282995\n",
      "Epoch:143, accuracy: 0.9000, Loss: 0.6800635784952419\n",
      "Epoch:144, accuracy: 0.9000, Loss: 0.6800398290210732\n",
      "Epoch:145, accuracy: 0.9000, Loss: 0.6800163094139284\n",
      "Epoch:146, accuracy: 0.9000, Loss: 0.6799930156994588\n",
      "Epoch:147, accuracy: 0.9000, Loss: 0.6799699440542967\n",
      "Epoch:148, accuracy: 0.9000, Loss: 0.6799470908335401\n",
      "Epoch:149, accuracy: 0.9000, Loss: 0.6799244524509569\n",
      "Epoch:150, accuracy: 0.9000, Loss: 0.6799020253635276\n",
      "Epoch:151, accuracy: 0.9000, Loss: 0.6798798061412399\n",
      "Epoch:152, accuracy: 0.9000, Loss: 0.679857791470379\n",
      "Epoch:153, accuracy: 0.9000, Loss: 0.6798359781236831\n",
      "Epoch:154, accuracy: 0.9000, Loss: 0.6798143629362005\n",
      "Epoch:155, accuracy: 0.9000, Loss: 0.6797929428084644\n",
      "Epoch:156, accuracy: 0.9000, Loss: 0.6797717147410681\n",
      "Epoch:157, accuracy: 0.9000, Loss: 0.6797506758238775\n",
      "Epoch:158, accuracy: 0.9000, Loss: 0.679729823186622\n",
      "Epoch:159, accuracy: 0.9000, Loss: 0.6797091540249643\n",
      "Epoch:160, accuracy: 0.9000, Loss: 0.6796886656377429\n",
      "Epoch:161, accuracy: 0.9000, Loss: 0.679668355376808\n",
      "Epoch:162, accuracy: 0.9000, Loss: 0.6796482206214112\n",
      "Epoch:163, accuracy: 0.9000, Loss: 0.6796282588349849\n",
      "Epoch:164, accuracy: 0.9000, Loss: 0.6796084675687645\n",
      "Epoch:165, accuracy: 0.9000, Loss: 0.6795888444059324\n",
      "Epoch:166, accuracy: 0.9000, Loss: 0.6795693869615762\n",
      "Epoch:167, accuracy: 0.9000, Loss: 0.6795500929210438\n",
      "Epoch:168, accuracy: 0.9000, Loss: 0.6795309600397286\n",
      "Epoch:169, accuracy: 0.9000, Loss: 0.6795119861182863\n",
      "Epoch:170, accuracy: 0.9000, Loss: 0.6794931689885144\n",
      "Epoch:171, accuracy: 0.9000, Loss: 0.6794745065244088\n",
      "Epoch:172, accuracy: 0.9000, Loss: 0.6794559966632815\n",
      "Epoch:173, accuracy: 0.9000, Loss: 0.6794376373944613\n",
      "Epoch:174, accuracy: 0.9000, Loss: 0.6794194267306142\n",
      "Epoch:175, accuracy: 0.9000, Loss: 0.6794013627211808\n",
      "Epoch:176, accuracy: 0.9000, Loss: 0.6793834434750708\n",
      "Epoch:177, accuracy: 0.9000, Loss: 0.679365667138619\n",
      "Epoch:178, accuracy: 0.9000, Loss: 0.6793480318786103\n",
      "Epoch:179, accuracy: 0.9000, Loss: 0.6793305359056219\n",
      "Epoch:180, accuracy: 0.9000, Loss: 0.679313177479032\n",
      "Epoch:181, accuracy: 0.9000, Loss: 0.6792959548827431\n",
      "Epoch:182, accuracy: 0.9000, Loss: 0.6792788664244938\n",
      "Epoch:183, accuracy: 0.9000, Loss: 0.6792619104537821\n",
      "Epoch:184, accuracy: 0.9000, Loss: 0.6792450853588462\n",
      "Epoch:185, accuracy: 0.9000, Loss: 0.6792283895521543\n",
      "Epoch:186, accuracy: 0.9000, Loss: 0.6792118214683696\n",
      "Epoch:187, accuracy: 0.9000, Loss: 0.6791953795733996\n",
      "Epoch:188, accuracy: 0.9000, Loss: 0.6791790623699171\n",
      "Epoch:189, accuracy: 0.9000, Loss: 0.6791628683882287\n",
      "Epoch:190, accuracy: 0.9000, Loss: 0.6791467961747325\n",
      "Epoch:191, accuracy: 0.9000, Loss: 0.6791308443000517\n",
      "Epoch:192, accuracy: 0.9000, Loss: 0.6791150113703728\n",
      "Epoch:193, accuracy: 0.9000, Loss: 0.6790992960166136\n",
      "Epoch:194, accuracy: 0.9000, Loss: 0.6790836968835754\n",
      "Epoch:195, accuracy: 0.9000, Loss: 0.6790682126405762\n",
      "Epoch:196, accuracy: 0.9000, Loss: 0.6790528419871796\n",
      "Epoch:197, accuracy: 0.9000, Loss: 0.6790375836414946\n",
      "Epoch:198, accuracy: 0.9000, Loss: 0.6790224363370923\n",
      "Epoch:199, accuracy: 0.9000, Loss: 0.6790073988322292\n",
      "Epoch:200, accuracy: 0.9000, Loss: 0.6789924699093294\n",
      "Epoch:201, accuracy: 0.9000, Loss: 0.6789776483668805\n",
      "Epoch:202, accuracy: 0.9000, Loss: 0.678962933019563\n",
      "Epoch:203, accuracy: 0.9000, Loss: 0.678948322703448\n",
      "Epoch:204, accuracy: 0.9000, Loss: 0.6789338162758212\n",
      "Epoch:205, accuracy: 0.9000, Loss: 0.6789194126102592\n",
      "Epoch:206, accuracy: 0.9000, Loss: 0.6789051105942358\n",
      "Epoch:207, accuracy: 0.9000, Loss: 0.6788909091330388\n",
      "Epoch:208, accuracy: 0.9000, Loss: 0.6788768071528873\n",
      "Epoch:209, accuracy: 0.9000, Loss: 0.6788628035958323\n",
      "Epoch:210, accuracy: 0.9000, Loss: 0.678848897415148\n",
      "Epoch:211, accuracy: 0.9000, Loss: 0.6788350875802127\n",
      "Epoch:212, accuracy: 0.9000, Loss: 0.6788213730800622\n",
      "Epoch:213, accuracy: 0.9000, Loss: 0.6788077529176141\n",
      "Epoch:214, accuracy: 0.9000, Loss: 0.6787942261065704\n",
      "Epoch:215, accuracy: 0.9000, Loss: 0.6787807916763273\n",
      "Epoch:216, accuracy: 0.9000, Loss: 0.6787674486730697\n",
      "Epoch:217, accuracy: 0.9000, Loss: 0.6787541961548926\n",
      "Epoch:218, accuracy: 0.9000, Loss: 0.6787410331911818\n",
      "Epoch:219, accuracy: 0.9000, Loss: 0.6787279588660591\n",
      "Epoch:220, accuracy: 0.9000, Loss: 0.6787149722779446\n",
      "Epoch:221, accuracy: 0.9000, Loss: 0.6787020725364417\n",
      "Epoch:222, accuracy: 0.9000, Loss: 0.678689258762187\n",
      "Epoch:223, accuracy: 0.9000, Loss: 0.6786765300888439\n",
      "Epoch:224, accuracy: 0.9000, Loss: 0.6786638856631648\n",
      "Epoch:225, accuracy: 0.9000, Loss: 0.6786513246428024\n",
      "Epoch:226, accuracy: 0.9000, Loss: 0.6786388461952773\n",
      "Epoch:227, accuracy: 0.9000, Loss: 0.6786264494997475\n",
      "Epoch:228, accuracy: 0.9000, Loss: 0.6786141337479922\n",
      "Epoch:229, accuracy: 0.9000, Loss: 0.6786018981420534\n",
      "Epoch:230, accuracy: 0.9000, Loss: 0.6785897418927179\n",
      "Epoch:231, accuracy: 0.9000, Loss: 0.6785776642216199\n",
      "Epoch:232, accuracy: 0.9000, Loss: 0.6785656643622135\n",
      "Epoch:233, accuracy: 0.9000, Loss: 0.6785537415572873\n",
      "Epoch:234, accuracy: 0.9000, Loss: 0.678541895057985\n",
      "Epoch:235, accuracy: 0.9000, Loss: 0.6785301241258035\n",
      "Epoch:236, accuracy: 0.9000, Loss: 0.6785184280328187\n",
      "Epoch:237, accuracy: 0.9000, Loss: 0.6785068060596143\n",
      "Epoch:238, accuracy: 0.9000, Loss: 0.6784952574950404\n",
      "Epoch:239, accuracy: 0.9000, Loss: 0.6784837816376506\n",
      "Epoch:240, accuracy: 0.9000, Loss: 0.6784723777954821\n",
      "Epoch:241, accuracy: 0.9000, Loss: 0.6784610452846389\n",
      "Epoch:242, accuracy: 0.9000, Loss: 0.6784497834292124\n",
      "Epoch:243, accuracy: 0.9000, Loss: 0.6784385915621585\n",
      "Epoch:244, accuracy: 0.9000, Loss: 0.6784274690252066\n",
      "Epoch:245, accuracy: 0.9000, Loss: 0.6784164151678583\n",
      "Epoch:246, accuracy: 0.9000, Loss: 0.6784054293470684\n",
      "Epoch:247, accuracy: 0.9000, Loss: 0.6783945109279312\n",
      "Epoch:248, accuracy: 0.9000, Loss: 0.6783836592838761\n",
      "Epoch:249, accuracy: 0.9000, Loss: 0.678372873795722\n",
      "Epoch:250, accuracy: 0.9000, Loss: 0.6783621538511682\n",
      "Epoch:251, accuracy: 0.9000, Loss: 0.6783514988455297\n",
      "Epoch:252, accuracy: 0.9000, Loss: 0.6783409081820032\n",
      "Epoch:253, accuracy: 0.9000, Loss: 0.678330381270694\n",
      "Epoch:254, accuracy: 0.9000, Loss: 0.6783199175282013\n",
      "Epoch:255, accuracy: 0.9000, Loss: 0.6783095163783537\n",
      "Epoch:256, accuracy: 0.9000, Loss: 0.6782991772523023\n",
      "Epoch:257, accuracy: 0.9000, Loss: 0.6782888995876286\n",
      "Epoch:258, accuracy: 0.9000, Loss: 0.6782786828281496\n",
      "Epoch:259, accuracy: 0.9000, Loss: 0.6782685264245221\n",
      "Epoch:260, accuracy: 0.9000, Loss: 0.6782584298341715\n",
      "Epoch:261, accuracy: 0.9000, Loss: 0.6782483925205975\n",
      "Epoch:262, accuracy: 0.9000, Loss: 0.6782384139532889\n",
      "Epoch:263, accuracy: 0.9000, Loss: 0.6782284936081278\n",
      "Epoch:264, accuracy: 0.9000, Loss: 0.6782186309672955\n",
      "Epoch:265, accuracy: 0.9000, Loss: 0.6782088255187773\n",
      "Epoch:266, accuracy: 0.9000, Loss: 0.6781990767562565\n",
      "Epoch:267, accuracy: 0.9000, Loss: 0.6781893841793737\n",
      "Epoch:268, accuracy: 0.9000, Loss: 0.6781797472937086\n",
      "Epoch:269, accuracy: 0.9000, Loss: 0.6781701656103873\n",
      "Epoch:270, accuracy: 0.9000, Loss: 0.6781606386459073\n",
      "Epoch:271, accuracy: 0.9000, Loss: 0.6781511659223488\n",
      "Epoch:272, accuracy: 0.9000, Loss: 0.6781417469674204\n",
      "Epoch:273, accuracy: 0.9000, Loss: 0.678132381314099\n",
      "Epoch:274, accuracy: 0.9000, Loss: 0.6781230685004314\n",
      "Epoch:275, accuracy: 0.9000, Loss: 0.6781138080697422\n",
      "Epoch:276, accuracy: 0.9000, Loss: 0.6781045995706823\n",
      "Epoch:277, accuracy: 0.9000, Loss: 0.6780954425568758\n",
      "Epoch:278, accuracy: 0.9000, Loss: 0.6780863365867532\n",
      "Epoch:279, accuracy: 0.9000, Loss: 0.6780772812237572\n",
      "Epoch:280, accuracy: 0.9000, Loss: 0.6780682760363531\n",
      "Epoch:281, accuracy: 0.9000, Loss: 0.6780593205977065\n",
      "Epoch:282, accuracy: 0.9000, Loss: 0.6780504144855655\n",
      "Epoch:283, accuracy: 0.9000, Loss: 0.6780415572824259\n",
      "Epoch:284, accuracy: 0.9000, Loss: 0.6780327485755077\n",
      "Epoch:285, accuracy: 0.9000, Loss: 0.678023987956494\n",
      "Epoch:286, accuracy: 0.9000, Loss: 0.6780152750214424\n",
      "Epoch:287, accuracy: 0.9000, Loss: 0.6780066093708962\n",
      "Epoch:288, accuracy: 0.9000, Loss: 0.6779979906098506\n",
      "Epoch:289, accuracy: 0.9000, Loss: 0.6779894183475526\n",
      "Epoch:290, accuracy: 0.9000, Loss: 0.6779808921974179\n",
      "Epoch:291, accuracy: 0.9000, Loss: 0.6779724117770927\n",
      "Epoch:292, accuracy: 0.9000, Loss: 0.6779639767084337\n",
      "Epoch:293, accuracy: 0.9000, Loss: 0.6779555866173526\n",
      "Epoch:294, accuracy: 0.9000, Loss: 0.6779472411337213\n",
      "Epoch:295, accuracy: 0.9000, Loss: 0.6779389398914113\n",
      "Epoch:296, accuracy: 0.9000, Loss: 0.6779306825282887\n",
      "Epoch:297, accuracy: 0.9000, Loss: 0.677922468686079\n",
      "Epoch:298, accuracy: 0.9000, Loss: 0.6779142980102656\n",
      "Epoch:299, accuracy: 0.9000, Loss: 0.6779061701501219\n",
      "Epoch:300, accuracy: 0.9000, Loss: 0.6778980847587159\n",
      "Epoch:301, accuracy: 0.9000, Loss: 0.6778900414927851\n",
      "Epoch:302, accuracy: 0.9000, Loss: 0.6778820400126365\n",
      "Epoch:303, accuracy: 0.9000, Loss: 0.6778740799821754\n",
      "Epoch:304, accuracy: 0.9000, Loss: 0.6778661610689085\n",
      "Epoch:305, accuracy: 0.9000, Loss: 0.6778582829438289\n",
      "Epoch:306, accuracy: 0.9000, Loss: 0.6778504452813277\n",
      "Epoch:307, accuracy: 0.9000, Loss: 0.677842647759215\n",
      "Epoch:308, accuracy: 0.9000, Loss: 0.6778348900587171\n",
      "Epoch:309, accuracy: 0.9000, Loss: 0.6778271718643742\n",
      "Epoch:310, accuracy: 0.9000, Loss: 0.6778194928639651\n",
      "Epoch:311, accuracy: 0.9000, Loss: 0.6778118527485189\n",
      "Epoch:312, accuracy: 0.9000, Loss: 0.6778042512123026\n",
      "Epoch:313, accuracy: 0.9000, Loss: 0.6777966879527373\n",
      "Epoch:314, accuracy: 0.9000, Loss: 0.6777891626703326\n",
      "Epoch:315, accuracy: 0.9000, Loss: 0.6777816750686859\n",
      "Epoch:316, accuracy: 0.9000, Loss: 0.6777742248544689\n",
      "Epoch:317, accuracy: 0.9000, Loss: 0.6777668117373566\n",
      "Epoch:318, accuracy: 0.9000, Loss: 0.6777594354299674\n",
      "Epoch:319, accuracy: 0.9000, Loss: 0.6777520956478542\n",
      "Epoch:320, accuracy: 0.9000, Loss: 0.6777447921094903\n",
      "Epoch:321, accuracy: 0.9000, Loss: 0.6777375245362124\n",
      "Epoch:322, accuracy: 0.9000, Loss: 0.677730292652162\n",
      "Epoch:323, accuracy: 0.9000, Loss: 0.6777230961842687\n",
      "Epoch:324, accuracy: 0.9000, Loss: 0.6777159348622404\n",
      "Epoch:325, accuracy: 0.9000, Loss: 0.6777088084185132\n",
      "Epoch:326, accuracy: 0.9000, Loss: 0.677701716588197\n",
      "Epoch:327, accuracy: 0.9000, Loss: 0.6776946591090532\n",
      "Epoch:328, accuracy: 0.9000, Loss: 0.6776876357214869\n",
      "Epoch:329, accuracy: 0.9000, Loss: 0.6776806461685038\n",
      "Epoch:330, accuracy: 0.9000, Loss: 0.6776736901956565\n",
      "Epoch:331, accuracy: 0.9000, Loss: 0.6776667675510242\n",
      "Epoch:332, accuracy: 0.9000, Loss: 0.6776598779852012\n",
      "Epoch:333, accuracy: 0.9000, Loss: 0.6776530212512598\n",
      "Epoch:334, accuracy: 0.9000, Loss: 0.6776461971047016\n",
      "Epoch:335, accuracy: 0.9000, Loss: 0.6776394053034341\n",
      "Epoch:336, accuracy: 0.9000, Loss: 0.6776326456077614\n",
      "Epoch:337, accuracy: 0.9000, Loss: 0.677625917780348\n",
      "Epoch:338, accuracy: 0.9000, Loss: 0.6776192215861756\n",
      "Epoch:339, accuracy: 0.9000, Loss: 0.6776125567925195\n",
      "Epoch:340, accuracy: 0.9000, Loss: 0.6776059231689372\n",
      "Epoch:341, accuracy: 0.9000, Loss: 0.6775993204872373\n",
      "Epoch:342, accuracy: 0.9000, Loss: 0.6775927485214387\n",
      "Epoch:343, accuracy: 0.9000, Loss: 0.6775862070477476\n",
      "Epoch:344, accuracy: 0.9000, Loss: 0.6775796958445448\n",
      "Epoch:345, accuracy: 0.9000, Loss: 0.6775732146923573\n",
      "Epoch:346, accuracy: 0.9000, Loss: 0.6775667633738226\n",
      "Epoch:347, accuracy: 0.9000, Loss: 0.677560341673664\n",
      "Epoch:348, accuracy: 0.9000, Loss: 0.6775539493786777\n",
      "Epoch:349, accuracy: 0.9000, Loss: 0.6775475862777068\n",
      "Epoch:350, accuracy: 0.9000, Loss: 0.6775412521616101\n",
      "Epoch:351, accuracy: 0.9000, Loss: 0.677534946823237\n",
      "Epoch:352, accuracy: 0.9000, Loss: 0.6775286700574124\n",
      "Epoch:353, accuracy: 0.9000, Loss: 0.6775224216609154\n",
      "Epoch:354, accuracy: 0.9000, Loss: 0.677516201432449\n",
      "Epoch:355, accuracy: 0.9000, Loss: 0.6775100091726176\n",
      "Epoch:356, accuracy: 0.9000, Loss: 0.6775038446839091\n",
      "Epoch:357, accuracy: 0.9000, Loss: 0.6774977077706773\n",
      "Epoch:358, accuracy: 0.9000, Loss: 0.6774915982391136\n",
      "Epoch:359, accuracy: 0.9000, Loss: 0.6774855158972247\n",
      "Epoch:360, accuracy: 0.9000, Loss: 0.6774794605548156\n",
      "Epoch:361, accuracy: 0.9000, Loss: 0.6774734320234717\n",
      "Epoch:362, accuracy: 0.9000, Loss: 0.6774674301165353\n",
      "Epoch:363, accuracy: 0.9000, Loss: 0.677461454649082\n",
      "Epoch:364, accuracy: 0.9000, Loss: 0.6774555054379039\n",
      "Epoch:365, accuracy: 0.9000, Loss: 0.6774495823014951\n",
      "Epoch:366, accuracy: 0.9000, Loss: 0.6774436850600276\n",
      "Epoch:367, accuracy: 0.9000, Loss: 0.6774378135353292\n",
      "Epoch:368, accuracy: 0.9000, Loss: 0.6774319675508684\n",
      "Epoch:369, accuracy: 0.9000, Loss: 0.6774261469317389\n",
      "Epoch:370, accuracy: 0.9000, Loss: 0.6774203515046381\n",
      "Epoch:371, accuracy: 0.9000, Loss: 0.6774145810978471\n",
      "Epoch:372, accuracy: 0.9000, Loss: 0.6774088355412147\n",
      "Epoch:373, accuracy: 0.9000, Loss: 0.6774031146661412\n",
      "Epoch:374, accuracy: 0.9000, Loss: 0.6773974183055623\n",
      "Epoch:375, accuracy: 0.9000, Loss: 0.677391746293927\n",
      "Epoch:376, accuracy: 0.9000, Loss: 0.6773860984671826\n",
      "Epoch:377, accuracy: 0.9000, Loss: 0.6773804746627607\n",
      "Epoch:378, accuracy: 0.9000, Loss: 0.6773748747195601\n",
      "Epoch:379, accuracy: 0.9000, Loss: 0.677369298477928\n",
      "Epoch:380, accuracy: 0.9000, Loss: 0.6773637457796438\n",
      "Epoch:381, accuracy: 0.9000, Loss: 0.6773582164679064\n",
      "Epoch:382, accuracy: 0.9000, Loss: 0.6773527103873186\n",
      "Epoch:383, accuracy: 0.9000, Loss: 0.6773472273838685\n",
      "Epoch:384, accuracy: 0.9000, Loss: 0.677341767304915\n",
      "Epoch:385, accuracy: 0.9000, Loss: 0.6773363299991746\n",
      "Epoch:386, accuracy: 0.9000, Loss: 0.6773309153167066\n",
      "Epoch:387, accuracy: 0.9000, Loss: 0.677325523108897\n",
      "Epoch:388, accuracy: 0.9000, Loss: 0.6773201532284431\n",
      "Epoch:389, accuracy: 0.9000, Loss: 0.6773148055293414\n",
      "Epoch:390, accuracy: 0.9000, Loss: 0.6773094798668742\n",
      "Epoch:391, accuracy: 0.9000, Loss: 0.6773041760975927\n",
      "Epoch:392, accuracy: 0.9000, Loss: 0.677298894079305\n",
      "Epoch:393, accuracy: 0.9000, Loss: 0.6772936336710619\n",
      "Epoch:394, accuracy: 0.9000, Loss: 0.6772883947331452\n",
      "Epoch:395, accuracy: 0.9000, Loss: 0.6772831771270524\n",
      "Epoch:396, accuracy: 0.9000, Loss: 0.6772779807154843\n",
      "Epoch:397, accuracy: 0.9000, Loss: 0.6772728053623318\n",
      "Epoch:398, accuracy: 0.9000, Loss: 0.6772676509326645\n",
      "Epoch:399, accuracy: 0.9000, Loss: 0.6772625172927167\n",
      "Epoch:400, accuracy: 0.9000, Loss: 0.6772574043098758\n",
      "Epoch:401, accuracy: 0.9000, Loss: 0.6772523118526687\n",
      "Epoch:402, accuracy: 0.9000, Loss: 0.6772472397907513\n",
      "Epoch:403, accuracy: 0.9000, Loss: 0.6772421879948965\n",
      "Epoch:404, accuracy: 0.9000, Loss: 0.6772371563369812\n",
      "Epoch:405, accuracy: 0.9000, Loss: 0.6772321446899745\n",
      "Epoch:406, accuracy: 0.9000, Loss: 0.6772271529279278\n",
      "Epoch:407, accuracy: 0.9000, Loss: 0.6772221809259622\n",
      "Epoch:408, accuracy: 0.9000, Loss: 0.6772172285602578\n",
      "Epoch:409, accuracy: 0.9000, Loss: 0.6772122957080418\n",
      "Epoch:410, accuracy: 0.9000, Loss: 0.6772073822475776\n",
      "Epoch:411, accuracy: 0.9000, Loss: 0.6772024880581562\n",
      "Epoch:412, accuracy: 0.9000, Loss: 0.6771976130200823\n",
      "Epoch:413, accuracy: 0.9000, Loss: 0.677192757014665\n",
      "Epoch:414, accuracy: 0.9000, Loss: 0.677187919924208\n",
      "Epoch:415, accuracy: 0.9000, Loss: 0.6771831016319989\n",
      "Epoch:416, accuracy: 0.9000, Loss: 0.6771783020222988\n",
      "Epoch:417, accuracy: 0.9000, Loss: 0.6771735209803317\n",
      "Epoch:418, accuracy: 0.9000, Loss: 0.6771687583922757\n",
      "Epoch:419, accuracy: 0.9000, Loss: 0.6771640141452526\n",
      "Epoch:420, accuracy: 0.9000, Loss: 0.6771592881273185\n",
      "Epoch:421, accuracy: 0.9000, Loss: 0.6771545802274535\n",
      "Epoch:422, accuracy: 0.9000, Loss: 0.6771498903355522\n",
      "Epoch:423, accuracy: 0.9000, Loss: 0.6771452183424159\n",
      "Epoch:424, accuracy: 0.9000, Loss: 0.6771405641397419\n",
      "Epoch:425, accuracy: 0.9000, Loss: 0.6771359276201144\n",
      "Epoch:426, accuracy: 0.9000, Loss: 0.6771313086769958\n",
      "Epoch:427, accuracy: 0.9000, Loss: 0.677126707204718\n",
      "Epoch:428, accuracy: 0.9000, Loss: 0.6771221230984735\n",
      "Epoch:429, accuracy: 0.9000, Loss: 0.6771175562543064\n",
      "Epoch:430, accuracy: 0.9000, Loss: 0.6771130065691036\n",
      "Epoch:431, accuracy: 0.9000, Loss: 0.6771084739405872\n",
      "Epoch:432, accuracy: 0.9000, Loss: 0.6771039582673053\n",
      "Epoch:433, accuracy: 0.9000, Loss: 0.6770994594486242\n",
      "Epoch:434, accuracy: 0.9000, Loss: 0.6770949773847195\n",
      "Epoch:435, accuracy: 0.9000, Loss: 0.6770905119765687\n",
      "Epoch:436, accuracy: 0.9000, Loss: 0.677086063125943\n",
      "Epoch:437, accuracy: 0.9000, Loss: 0.6770816307353995\n",
      "Epoch:438, accuracy: 0.9000, Loss: 0.6770772147082726\n",
      "Epoch:439, accuracy: 0.9000, Loss: 0.6770728149486674\n",
      "Epoch:440, accuracy: 0.9000, Loss: 0.6770684313614513\n",
      "Epoch:441, accuracy: 0.9000, Loss: 0.6770640638522469\n",
      "Epoch:442, accuracy: 0.9000, Loss: 0.677059712327424\n",
      "Epoch:443, accuracy: 0.9000, Loss: 0.6770553766940928\n",
      "Epoch:444, accuracy: 0.9000, Loss: 0.6770510568600961\n",
      "Epoch:445, accuracy: 0.9000, Loss: 0.6770467527340028\n",
      "Epoch:446, accuracy: 0.9000, Loss: 0.6770424642250998\n",
      "Epoch:447, accuracy: 0.9000, Loss: 0.6770381912433858\n",
      "Epoch:448, accuracy: 0.9000, Loss: 0.6770339336995641\n",
      "Epoch:449, accuracy: 0.9000, Loss: 0.6770296915050353\n",
      "Epoch:450, accuracy: 0.9000, Loss: 0.6770254645718913\n",
      "Epoch:451, accuracy: 0.9000, Loss: 0.6770212528129078\n",
      "Epoch:452, accuracy: 0.9000, Loss: 0.6770170561415382\n",
      "Epoch:453, accuracy: 0.9000, Loss: 0.6770128744719067\n",
      "Epoch:454, accuracy: 0.9000, Loss: 0.6770087077188022\n",
      "Epoch:455, accuracy: 0.9000, Loss: 0.6770045557976712\n",
      "Epoch:456, accuracy: 0.9000, Loss: 0.6770004186246124\n",
      "Epoch:457, accuracy: 0.9000, Loss: 0.6769962961163695\n",
      "Epoch:458, accuracy: 0.9000, Loss: 0.6769921881903256\n",
      "Epoch:459, accuracy: 0.9000, Loss: 0.6769880947644971\n",
      "Epoch:460, accuracy: 0.9000, Loss: 0.6769840157575266\n",
      "Epoch:461, accuracy: 0.9000, Loss: 0.6769799510886791\n",
      "Epoch:462, accuracy: 0.9000, Loss: 0.6769759006778336\n",
      "Epoch:463, accuracy: 0.9000, Loss: 0.6769718644454787\n",
      "Epoch:464, accuracy: 0.9000, Loss: 0.6769678423127069\n",
      "Epoch:465, accuracy: 0.9000, Loss: 0.6769638342012076\n",
      "Epoch:466, accuracy: 0.9000, Loss: 0.6769598400332633\n",
      "Epoch:467, accuracy: 0.9000, Loss: 0.676955859731742\n",
      "Epoch:468, accuracy: 0.9000, Loss: 0.6769518932200935\n",
      "Epoch:469, accuracy: 0.9000, Loss: 0.6769479404223424\n",
      "Epoch:470, accuracy: 0.9000, Loss: 0.6769440012630843\n",
      "Epoch:471, accuracy: 0.9000, Loss: 0.6769400756674784\n",
      "Epoch:472, accuracy: 0.9000, Loss: 0.6769361635612443\n",
      "Epoch:473, accuracy: 0.9000, Loss: 0.6769322648706552\n",
      "Epoch:474, accuracy: 0.9000, Loss: 0.6769283795225335\n",
      "Epoch:475, accuracy: 0.9000, Loss: 0.676924507444246\n",
      "Epoch:476, accuracy: 0.9000, Loss: 0.6769206485636978\n",
      "Epoch:477, accuracy: 0.9000, Loss: 0.676916802809328\n",
      "Epoch:478, accuracy: 0.9000, Loss: 0.6769129701101047\n",
      "Epoch:479, accuracy: 0.9000, Loss: 0.6769091503955198\n",
      "Epoch:480, accuracy: 0.9000, Loss: 0.6769053435955847\n",
      "Epoch:481, accuracy: 0.9000, Loss: 0.6769015496408253\n",
      "Epoch:482, accuracy: 0.9000, Loss: 0.6768977684622762\n",
      "Epoch:483, accuracy: 0.9000, Loss: 0.6768939999914787\n",
      "Epoch:484, accuracy: 0.9000, Loss: 0.6768902441604726\n",
      "Epoch:485, accuracy: 0.9000, Loss: 0.6768865009017947\n",
      "Epoch:486, accuracy: 0.9000, Loss: 0.6768827701484725\n",
      "Epoch:487, accuracy: 0.9000, Loss: 0.6768790518340205\n",
      "Epoch:488, accuracy: 0.9000, Loss: 0.6768753458924354\n",
      "Epoch:489, accuracy: 0.9000, Loss: 0.6768716522581918\n",
      "Epoch:490, accuracy: 0.9000, Loss: 0.6768679708662378\n",
      "Epoch:491, accuracy: 0.9000, Loss: 0.6768643016519913\n",
      "Epoch:492, accuracy: 0.9000, Loss: 0.6768606445513348\n",
      "Epoch:493, accuracy: 0.9000, Loss: 0.6768569995006115\n",
      "Epoch:494, accuracy: 0.9000, Loss: 0.6768533664366219\n",
      "Epoch:495, accuracy: 0.9000, Loss: 0.6768497452966187\n",
      "Epoch:496, accuracy: 0.9000, Loss: 0.676846136018303\n",
      "Epoch:497, accuracy: 0.9000, Loss: 0.6768425385398211\n",
      "Epoch:498, accuracy: 0.9000, Loss: 0.676838952799759\n",
      "Epoch:499, accuracy: 0.9000, Loss: 0.6768353787371398\n",
      "Epoch:500, accuracy: 0.9000, Loss: 0.6768318162914192\n",
      "Epoch:501, accuracy: 0.9000, Loss: 0.6768282654024815\n",
      "Epoch:502, accuracy: 0.9000, Loss: 0.6768247260106367\n",
      "Epoch:503, accuracy: 0.9000, Loss: 0.6768211980566151\n",
      "Epoch:504, accuracy: 0.9000, Loss: 0.6768176814815654\n",
      "Epoch:505, accuracy: 0.9000, Loss: 0.6768141762270498\n",
      "Epoch:506, accuracy: 0.9000, Loss: 0.6768106822350404\n",
      "Epoch:507, accuracy: 0.9000, Loss: 0.6768071994479167\n",
      "Epoch:508, accuracy: 0.9000, Loss: 0.67680372780846\n",
      "Epoch:509, accuracy: 0.9000, Loss: 0.6768002672598523\n",
      "Epoch:510, accuracy: 0.9000, Loss: 0.6767968177456706\n",
      "Epoch:511, accuracy: 0.9000, Loss: 0.676793379209885\n",
      "Epoch:512, accuracy: 0.9000, Loss: 0.6767899515968541\n",
      "Epoch:513, accuracy: 0.9000, Loss: 0.6767865348513229\n",
      "Epoch:514, accuracy: 0.9000, Loss: 0.6767831289184181\n",
      "Epoch:515, accuracy: 0.9000, Loss: 0.6767797337436455\n",
      "Epoch:516, accuracy: 0.9000, Loss: 0.6767763492728867\n",
      "Epoch:517, accuracy: 0.9000, Loss: 0.6767729754523957\n",
      "Epoch:518, accuracy: 0.9000, Loss: 0.6767696122287961\n",
      "Epoch:519, accuracy: 0.9000, Loss: 0.6767662595490764\n",
      "Epoch:520, accuracy: 0.9000, Loss: 0.6767629173605894\n",
      "Epoch:521, accuracy: 0.9000, Loss: 0.6767595856110465\n",
      "Epoch:522, accuracy: 0.9000, Loss: 0.6767562642485168\n",
      "Epoch:523, accuracy: 0.9000, Loss: 0.6767529532214218\n",
      "Epoch:524, accuracy: 0.9000, Loss: 0.6767496524785346\n",
      "Epoch:525, accuracy: 0.9000, Loss: 0.6767463619689753\n",
      "Epoch:526, accuracy: 0.9000, Loss: 0.676743081642209\n",
      "Epoch:527, accuracy: 0.9000, Loss: 0.6767398114480421\n",
      "Epoch:528, accuracy: 0.9000, Loss: 0.67673655133662\n",
      "Epoch:529, accuracy: 0.9000, Loss: 0.6767333012584238\n",
      "Epoch:530, accuracy: 0.9000, Loss: 0.6767300611642679\n",
      "Epoch:531, accuracy: 0.9000, Loss: 0.6767268310052967\n",
      "Epoch:532, accuracy: 0.9000, Loss: 0.6767236107329824\n",
      "Epoch:533, accuracy: 0.9000, Loss: 0.6767204002991212\n",
      "Epoch:534, accuracy: 0.9000, Loss: 0.6767171996558318\n",
      "Epoch:535, accuracy: 0.9000, Loss: 0.6767140087555522\n",
      "Epoch:536, accuracy: 0.9000, Loss: 0.6767108275510364\n",
      "Epoch:537, accuracy: 0.9000, Loss: 0.6767076559953528\n",
      "Epoch:538, accuracy: 0.9000, Loss: 0.6767044940418808\n",
      "Epoch:539, accuracy: 0.9000, Loss: 0.6767013416443086\n",
      "Epoch:540, accuracy: 0.9000, Loss: 0.676698198756631\n",
      "Epoch:541, accuracy: 0.9000, Loss: 0.6766950653331448\n",
      "Epoch:542, accuracy: 0.9000, Loss: 0.6766919413284498\n",
      "Epoch:543, accuracy: 0.9000, Loss: 0.6766888266974428\n",
      "Epoch:544, accuracy: 0.9000, Loss: 0.6766857213953178\n",
      "Epoch:545, accuracy: 0.9000, Loss: 0.6766826253775613\n",
      "Epoch:546, accuracy: 0.9000, Loss: 0.676679538599952\n",
      "Epoch:547, accuracy: 0.9000, Loss: 0.6766764610185568\n",
      "Epoch:548, accuracy: 0.9000, Loss: 0.676673392589729\n",
      "Epoch:549, accuracy: 0.9000, Loss: 0.6766703332701065\n",
      "Epoch:550, accuracy: 0.9000, Loss: 0.6766672830166086\n",
      "Epoch:551, accuracy: 0.9000, Loss: 0.6766642417864336\n",
      "Epoch:552, accuracy: 0.9000, Loss: 0.6766612095370574\n",
      "Epoch:553, accuracy: 0.9000, Loss: 0.6766581862262309\n",
      "Epoch:554, accuracy: 0.9000, Loss: 0.6766551718119773\n",
      "Epoch:555, accuracy: 0.9000, Loss: 0.6766521662525904\n",
      "Epoch:556, accuracy: 0.9000, Loss: 0.6766491695066323\n",
      "Epoch:557, accuracy: 0.9000, Loss: 0.676646181532931\n",
      "Epoch:558, accuracy: 0.9000, Loss: 0.6766432022905782\n",
      "Epoch:559, accuracy: 0.9000, Loss: 0.6766402317389277\n",
      "Epoch:560, accuracy: 0.9000, Loss: 0.676637269837593\n",
      "Epoch:561, accuracy: 0.9000, Loss: 0.6766343165464447\n",
      "Epoch:562, accuracy: 0.9000, Loss: 0.6766313718256096\n",
      "Epoch:563, accuracy: 0.9000, Loss: 0.6766284356354669\n",
      "Epoch:564, accuracy: 0.9000, Loss: 0.6766255079366486\n",
      "Epoch:565, accuracy: 0.9000, Loss: 0.6766225886900346\n",
      "Epoch:566, accuracy: 0.9000, Loss: 0.6766196778567531\n",
      "Epoch:567, accuracy: 0.9000, Loss: 0.6766167753981777\n",
      "Epoch:568, accuracy: 0.9000, Loss: 0.6766138812759254\n",
      "Epoch:569, accuracy: 0.9000, Loss: 0.6766109954518542\n",
      "Epoch:570, accuracy: 0.9000, Loss: 0.6766081178880623\n",
      "Epoch:571, accuracy: 0.9000, Loss: 0.6766052485468854\n",
      "Epoch:572, accuracy: 0.9000, Loss: 0.6766023873908953\n",
      "Epoch:573, accuracy: 0.9000, Loss: 0.6765995343828972\n",
      "Epoch:574, accuracy: 0.9000, Loss: 0.6765966894859291\n",
      "Epoch:575, accuracy: 0.9000, Loss: 0.6765938526632588\n",
      "Epoch:576, accuracy: 0.9000, Loss: 0.6765910238783824\n",
      "Epoch:577, accuracy: 0.9000, Loss: 0.6765882030950235\n",
      "Epoch:578, accuracy: 0.9000, Loss: 0.6765853902771299\n",
      "Epoch:579, accuracy: 0.9000, Loss: 0.6765825853888728\n",
      "Epoch:580, accuracy: 0.9000, Loss: 0.6765797883946449\n",
      "Epoch:581, accuracy: 0.9000, Loss: 0.676576999259058\n",
      "Epoch:582, accuracy: 0.9000, Loss: 0.6765742179469427\n",
      "Epoch:583, accuracy: 0.9000, Loss: 0.6765714444233454\n",
      "Epoch:584, accuracy: 0.9000, Loss: 0.6765686786535271\n",
      "Epoch:585, accuracy: 0.9000, Loss: 0.6765659206029617\n",
      "Epoch:586, accuracy: 0.9000, Loss: 0.6765631702373347\n",
      "Epoch:587, accuracy: 0.9000, Loss: 0.6765604275225408\n",
      "Epoch:588, accuracy: 0.9000, Loss: 0.676557692424683\n",
      "Epoch:589, accuracy: 0.9000, Loss: 0.6765549649100705\n",
      "Epoch:590, accuracy: 0.9000, Loss: 0.6765522449452174\n",
      "Epoch:591, accuracy: 0.9000, Loss: 0.6765495324968415\n",
      "Epoch:592, accuracy: 0.9000, Loss: 0.6765468275318616\n",
      "Epoch:593, accuracy: 0.9000, Loss: 0.6765441300173971\n",
      "Epoch:594, accuracy: 0.9000, Loss: 0.6765414399207658\n",
      "Epoch:595, accuracy: 0.9000, Loss: 0.6765387572094828\n",
      "Epoch:596, accuracy: 0.9000, Loss: 0.6765360818512586\n",
      "Epoch:597, accuracy: 0.9000, Loss: 0.676533413813998\n",
      "Epoch:598, accuracy: 0.9000, Loss: 0.6765307530657982\n",
      "Epoch:599, accuracy: 0.9000, Loss: 0.6765280995749479\n",
      "Epoch:600, accuracy: 0.9000, Loss: 0.6765254533099252\n",
      "Epoch:601, accuracy: 0.9000, Loss: 0.6765228142393969\n",
      "Epoch:602, accuracy: 0.9000, Loss: 0.676520182332216\n",
      "Epoch:603, accuracy: 0.9000, Loss: 0.6765175575574217\n",
      "Epoch:604, accuracy: 0.9000, Loss: 0.6765149398842368\n",
      "Epoch:605, accuracy: 0.9000, Loss: 0.676512329282067\n",
      "Epoch:606, accuracy: 0.9000, Loss: 0.6765097257204993\n",
      "Epoch:607, accuracy: 0.9000, Loss: 0.6765071291693\n",
      "Epoch:608, accuracy: 0.9000, Loss: 0.6765045395984155\n",
      "Epoch:609, accuracy: 0.9000, Loss: 0.6765019569779676\n",
      "Epoch:610, accuracy: 0.9000, Loss: 0.6764993812782556\n",
      "Epoch:611, accuracy: 0.9000, Loss: 0.6764968124697526\n",
      "Epoch:612, accuracy: 0.9000, Loss: 0.6764942505231052\n",
      "Epoch:613, accuracy: 0.9000, Loss: 0.6764916954091322\n",
      "Epoch:614, accuracy: 0.9000, Loss: 0.676489147098823\n",
      "Epoch:615, accuracy: 0.9000, Loss: 0.6764866055633366\n",
      "Epoch:616, accuracy: 0.9000, Loss: 0.6764840707740001\n",
      "Epoch:617, accuracy: 0.9000, Loss: 0.676481542702308\n",
      "Epoch:618, accuracy: 0.9000, Loss: 0.6764790213199199\n",
      "Epoch:619, accuracy: 0.9000, Loss: 0.6764765065986605\n",
      "Epoch:620, accuracy: 0.9000, Loss: 0.6764739985105177\n",
      "Epoch:621, accuracy: 0.9000, Loss: 0.6764714970276418\n",
      "Epoch:622, accuracy: 0.9000, Loss: 0.6764690021223435\n",
      "Epoch:623, accuracy: 0.9000, Loss: 0.6764665137670939\n",
      "Epoch:624, accuracy: 0.9000, Loss: 0.6764640319345219\n",
      "Epoch:625, accuracy: 0.9000, Loss: 0.6764615565974148\n",
      "Epoch:626, accuracy: 0.9000, Loss: 0.6764590877287155\n",
      "Epoch:627, accuracy: 0.9000, Loss: 0.6764566253015227\n",
      "Epoch:628, accuracy: 0.9000, Loss: 0.6764541692890884\n",
      "Epoch:629, accuracy: 0.9000, Loss: 0.6764517196648182\n",
      "Epoch:630, accuracy: 0.9000, Loss: 0.6764492764022692\n",
      "Epoch:631, accuracy: 0.9000, Loss: 0.6764468394751489\n",
      "Epoch:632, accuracy: 0.9000, Loss: 0.6764444088573154\n",
      "Epoch:633, accuracy: 0.9000, Loss: 0.676441984522774\n",
      "Epoch:634, accuracy: 0.9000, Loss: 0.6764395664456792\n",
      "Epoch:635, accuracy: 0.9000, Loss: 0.6764371546003302\n",
      "Epoch:636, accuracy: 0.9000, Loss: 0.6764347489611724\n",
      "Epoch:637, accuracy: 0.9000, Loss: 0.6764323495027957\n",
      "Epoch:638, accuracy: 0.9000, Loss: 0.6764299561999333\n",
      "Epoch:639, accuracy: 0.9000, Loss: 0.6764275690274598\n",
      "Epoch:640, accuracy: 0.9000, Loss: 0.6764251879603924\n",
      "Epoch:641, accuracy: 0.9000, Loss: 0.6764228129738872\n",
      "Epoch:642, accuracy: 0.9000, Loss: 0.6764204440432409\n",
      "Epoch:643, accuracy: 0.9000, Loss: 0.6764180811438871\n",
      "Epoch:644, accuracy: 0.9000, Loss: 0.6764157242513977\n",
      "Epoch:645, accuracy: 0.9000, Loss: 0.6764133733414809\n",
      "Epoch:646, accuracy: 0.9000, Loss: 0.6764110283899794\n",
      "Epoch:647, accuracy: 0.9000, Loss: 0.676408689372871\n",
      "Epoch:648, accuracy: 0.9000, Loss: 0.6764063562662669\n",
      "Epoch:649, accuracy: 0.9000, Loss: 0.6764040290464104\n",
      "Epoch:650, accuracy: 0.9000, Loss: 0.6764017076896769\n",
      "Epoch:651, accuracy: 0.9000, Loss: 0.676399392172572\n",
      "Epoch:652, accuracy: 0.9000, Loss: 0.6763970824717315\n",
      "Epoch:653, accuracy: 0.9000, Loss: 0.6763947785639195\n",
      "Epoch:654, accuracy: 0.9000, Loss: 0.6763924804260283\n",
      "Epoch:655, accuracy: 0.9000, Loss: 0.6763901880350771\n",
      "Epoch:656, accuracy: 0.9000, Loss: 0.6763879013682117\n",
      "Epoch:657, accuracy: 0.9000, Loss: 0.6763856204027026\n",
      "Epoch:658, accuracy: 0.9000, Loss: 0.6763833451159448\n",
      "Epoch:659, accuracy: 0.9000, Loss: 0.6763810754854569\n",
      "Epoch:660, accuracy: 0.9000, Loss: 0.6763788114888805\n",
      "Epoch:661, accuracy: 0.9000, Loss: 0.6763765531039784\n",
      "Epoch:662, accuracy: 0.9000, Loss: 0.6763743003086351\n",
      "Epoch:663, accuracy: 0.9000, Loss: 0.6763720530808547\n",
      "Epoch:664, accuracy: 0.9000, Loss: 0.6763698113987605\n",
      "Epoch:665, accuracy: 0.9000, Loss: 0.6763675752405949\n",
      "Epoch:666, accuracy: 0.9000, Loss: 0.6763653445847175\n",
      "Epoch:667, accuracy: 0.9000, Loss: 0.6763631194096054\n",
      "Epoch:668, accuracy: 0.9000, Loss: 0.6763608996938506\n",
      "Epoch:669, accuracy: 0.9000, Loss: 0.6763586854161616\n",
      "Epoch:670, accuracy: 0.9000, Loss: 0.6763564765553607\n",
      "Epoch:671, accuracy: 0.9000, Loss: 0.6763542730903843\n",
      "Epoch:672, accuracy: 0.9000, Loss: 0.6763520750002813\n",
      "Epoch:673, accuracy: 0.9000, Loss: 0.6763498822642131\n",
      "Epoch:674, accuracy: 0.9000, Loss: 0.6763476948614524\n",
      "Epoch:675, accuracy: 0.9000, Loss: 0.6763455127713824\n",
      "Epoch:676, accuracy: 0.9000, Loss: 0.6763433359734969\n",
      "Epoch:677, accuracy: 0.9000, Loss: 0.6763411644473979\n",
      "Epoch:678, accuracy: 0.9000, Loss: 0.6763389981727961\n",
      "Epoch:679, accuracy: 0.9000, Loss: 0.6763368371295103\n",
      "Epoch:680, accuracy: 0.9000, Loss: 0.6763346812974659\n",
      "Epoch:681, accuracy: 0.9000, Loss: 0.6763325306566944\n",
      "Epoch:682, accuracy: 0.9000, Loss: 0.6763303851873335\n",
      "Epoch:683, accuracy: 0.9000, Loss: 0.6763282448696251\n",
      "Epoch:684, accuracy: 0.9000, Loss: 0.6763261096839152\n",
      "Epoch:685, accuracy: 0.9000, Loss: 0.6763239796106533\n",
      "Epoch:686, accuracy: 0.9000, Loss: 0.6763218546303922\n",
      "Epoch:687, accuracy: 0.9000, Loss: 0.6763197347237856\n",
      "Epoch:688, accuracy: 0.9000, Loss: 0.6763176198715899\n",
      "Epoch:689, accuracy: 0.9000, Loss: 0.6763155100546612\n",
      "Epoch:690, accuracy: 0.9000, Loss: 0.6763134052539559\n",
      "Epoch:691, accuracy: 0.9000, Loss: 0.6763113054505301\n",
      "Epoch:692, accuracy: 0.9000, Loss: 0.6763092106255377\n",
      "Epoch:693, accuracy: 0.9000, Loss: 0.6763071207602316\n",
      "Epoch:694, accuracy: 0.9000, Loss: 0.6763050358359619\n",
      "Epoch:695, accuracy: 0.9000, Loss: 0.6763029558341749\n",
      "Epoch:696, accuracy: 0.9000, Loss: 0.6763008807364136\n",
      "Epoch:697, accuracy: 0.9000, Loss: 0.6762988105243161\n",
      "Epoch:698, accuracy: 0.9000, Loss: 0.6762967451796156\n",
      "Epoch:699, accuracy: 0.9000, Loss: 0.6762946846841394\n",
      "Epoch:700, accuracy: 0.9000, Loss: 0.6762926290198086\n",
      "Epoch:701, accuracy: 0.9000, Loss: 0.676290578168637\n",
      "Epoch:702, accuracy: 0.9000, Loss: 0.6762885321127311\n",
      "Epoch:703, accuracy: 0.9000, Loss: 0.676286490834289\n",
      "Epoch:704, accuracy: 0.9000, Loss: 0.6762844543155999\n",
      "Epoch:705, accuracy: 0.9000, Loss: 0.6762824225390438\n",
      "Epoch:706, accuracy: 0.9000, Loss: 0.6762803954870906\n",
      "Epoch:707, accuracy: 0.9000, Loss: 0.6762783731422997\n",
      "Epoch:708, accuracy: 0.9000, Loss: 0.6762763554873192\n",
      "Epoch:709, accuracy: 0.9000, Loss: 0.6762743425048858\n",
      "Epoch:710, accuracy: 0.9000, Loss: 0.6762723341778234\n",
      "Epoch:711, accuracy: 0.9000, Loss: 0.6762703304890434\n",
      "Epoch:712, accuracy: 0.9000, Loss: 0.6762683314215435\n",
      "Epoch:713, accuracy: 0.9000, Loss: 0.6762663369584078\n",
      "Epoch:714, accuracy: 0.9000, Loss: 0.6762643470828056\n",
      "Epoch:715, accuracy: 0.9000, Loss: 0.6762623617779908\n",
      "Epoch:716, accuracy: 0.9000, Loss: 0.6762603810273027\n",
      "Epoch:717, accuracy: 0.9000, Loss: 0.6762584048141627\n",
      "Epoch:718, accuracy: 0.9000, Loss: 0.6762564331220773\n",
      "Epoch:719, accuracy: 0.9000, Loss: 0.6762544659346347\n",
      "Epoch:720, accuracy: 0.9000, Loss: 0.6762525032355057\n",
      "Epoch:721, accuracy: 0.9000, Loss: 0.6762505450084423\n",
      "Epoch:722, accuracy: 0.9000, Loss: 0.6762485912372785\n",
      "Epoch:723, accuracy: 0.9000, Loss: 0.6762466419059282\n",
      "Epoch:724, accuracy: 0.9000, Loss: 0.6762446969983859\n",
      "Epoch:725, accuracy: 0.9000, Loss: 0.6762427564987256\n",
      "Epoch:726, accuracy: 0.9000, Loss: 0.6762408203911007\n",
      "Epoch:727, accuracy: 0.9000, Loss: 0.6762388886597428\n",
      "Epoch:728, accuracy: 0.9000, Loss: 0.6762369612889617\n",
      "Epoch:729, accuracy: 0.9000, Loss: 0.6762350382631453\n",
      "Epoch:730, accuracy: 0.9000, Loss: 0.6762331195667584\n",
      "Epoch:731, accuracy: 0.9000, Loss: 0.676231205184342\n",
      "Epoch:732, accuracy: 0.9000, Loss: 0.6762292951005142\n",
      "Epoch:733, accuracy: 0.9000, Loss: 0.6762273892999681\n",
      "Epoch:734, accuracy: 0.9000, Loss: 0.6762254877674723\n",
      "Epoch:735, accuracy: 0.9000, Loss: 0.6762235904878704\n",
      "Epoch:736, accuracy: 0.9000, Loss: 0.6762216974460801\n",
      "Epoch:737, accuracy: 0.9000, Loss: 0.6762198086270924\n",
      "Epoch:738, accuracy: 0.9000, Loss: 0.6762179240159727\n",
      "Epoch:739, accuracy: 0.9000, Loss: 0.6762160435978587\n",
      "Epoch:740, accuracy: 0.9000, Loss: 0.6762141673579608\n",
      "Epoch:741, accuracy: 0.9000, Loss: 0.676212295281561\n",
      "Epoch:742, accuracy: 0.9000, Loss: 0.6762104273540135\n",
      "Epoch:743, accuracy: 0.9000, Loss: 0.6762085635607432\n",
      "Epoch:744, accuracy: 0.9000, Loss: 0.676206703887246\n",
      "Epoch:745, accuracy: 0.9000, Loss: 0.6762048483190874\n",
      "Epoch:746, accuracy: 0.9000, Loss: 0.6762029968419039\n",
      "Epoch:747, accuracy: 0.9000, Loss: 0.6762011494414001\n",
      "Epoch:748, accuracy: 0.9000, Loss: 0.6761993061033504\n",
      "Epoch:749, accuracy: 0.9000, Loss: 0.6761974668135976\n",
      "Epoch:750, accuracy: 0.9000, Loss: 0.6761956315580523\n",
      "Epoch:751, accuracy: 0.9000, Loss: 0.6761938003226933\n",
      "Epoch:752, accuracy: 0.9000, Loss: 0.676191973093566\n",
      "Epoch:753, accuracy: 0.9000, Loss: 0.6761901498567836\n",
      "Epoch:754, accuracy: 0.9000, Loss: 0.6761883305985248\n",
      "Epoch:755, accuracy: 0.9000, Loss: 0.6761865153050352\n",
      "Epoch:756, accuracy: 0.9000, Loss: 0.6761847039626254\n",
      "Epoch:757, accuracy: 0.9000, Loss: 0.6761828965576717\n",
      "Epoch:758, accuracy: 0.9000, Loss: 0.6761810930766152\n",
      "Epoch:759, accuracy: 0.9000, Loss: 0.6761792935059613\n",
      "Epoch:760, accuracy: 0.9000, Loss: 0.6761774978322795\n",
      "Epoch:761, accuracy: 0.9000, Loss: 0.6761757060422033\n",
      "Epoch:762, accuracy: 0.9000, Loss: 0.6761739181224291\n",
      "Epoch:763, accuracy: 0.9000, Loss: 0.6761721340597165\n",
      "Epoch:764, accuracy: 0.9000, Loss: 0.6761703538408876\n",
      "Epoch:765, accuracy: 0.9000, Loss: 0.6761685774528267\n",
      "Epoch:766, accuracy: 0.9000, Loss: 0.6761668048824795\n",
      "Epoch:767, accuracy: 0.9000, Loss: 0.6761650361168539\n",
      "Epoch:768, accuracy: 0.9000, Loss: 0.6761632711430181\n",
      "Epoch:769, accuracy: 0.9000, Loss: 0.6761615099481015\n",
      "Epoch:770, accuracy: 0.9000, Loss: 0.6761597525192938\n",
      "Epoch:771, accuracy: 0.9000, Loss: 0.6761579988438442\n",
      "Epoch:772, accuracy: 0.9000, Loss: 0.6761562489090621\n",
      "Epoch:773, accuracy: 0.9000, Loss: 0.6761545027023158\n",
      "Epoch:774, accuracy: 0.9000, Loss: 0.6761527602110324\n",
      "Epoch:775, accuracy: 0.9000, Loss: 0.6761510214226981\n",
      "Epoch:776, accuracy: 0.9000, Loss: 0.6761492863248569\n",
      "Epoch:777, accuracy: 0.9000, Loss: 0.6761475549051108\n",
      "Epoch:778, accuracy: 0.9000, Loss: 0.676145827151119\n",
      "Epoch:779, accuracy: 0.9000, Loss: 0.6761441030505982\n",
      "Epoch:780, accuracy: 0.9000, Loss: 0.676142382591322\n",
      "Epoch:781, accuracy: 0.9000, Loss: 0.6761406657611204\n",
      "Epoch:782, accuracy: 0.9000, Loss: 0.6761389525478795\n",
      "Epoch:783, accuracy: 0.9000, Loss: 0.6761372429395411\n",
      "Epoch:784, accuracy: 0.9000, Loss: 0.6761355369241032\n",
      "Epoch:785, accuracy: 0.9000, Loss: 0.6761338344896182\n",
      "Epoch:786, accuracy: 0.9000, Loss: 0.6761321356241934\n",
      "Epoch:787, accuracy: 0.9000, Loss: 0.6761304403159916\n",
      "Epoch:788, accuracy: 0.9000, Loss: 0.6761287485532286\n",
      "Epoch:789, accuracy: 0.9000, Loss: 0.6761270603241747\n",
      "Epoch:790, accuracy: 0.9000, Loss: 0.6761253756171542\n",
      "Epoch:791, accuracy: 0.9000, Loss: 0.6761236944205437\n",
      "Epoch:792, accuracy: 0.9000, Loss: 0.6761220167227737\n",
      "Epoch:793, accuracy: 0.9000, Loss: 0.6761203425123268\n",
      "Epoch:794, accuracy: 0.9000, Loss: 0.6761186717777381\n",
      "Epoch:795, accuracy: 0.9000, Loss: 0.6761170045075947\n",
      "Epoch:796, accuracy: 0.9000, Loss: 0.6761153406905358\n",
      "Epoch:797, accuracy: 0.9000, Loss: 0.6761136803152515\n",
      "Epoch:798, accuracy: 0.9000, Loss: 0.6761120233704838\n",
      "Epoch:799, accuracy: 0.9000, Loss: 0.6761103698450248\n",
      "Epoch:800, accuracy: 0.9000, Loss: 0.6761087197277176\n",
      "Epoch:801, accuracy: 0.9000, Loss: 0.6761070730074553\n",
      "Epoch:802, accuracy: 0.9000, Loss: 0.6761054296731814\n",
      "Epoch:803, accuracy: 0.9000, Loss: 0.6761037897138887\n",
      "Epoch:804, accuracy: 0.9000, Loss: 0.6761021531186194\n",
      "Epoch:805, accuracy: 0.9000, Loss: 0.6761005198764656\n",
      "Epoch:806, accuracy: 0.9000, Loss: 0.676098889976567\n",
      "Epoch:807, accuracy: 0.9000, Loss: 0.6760972634081128\n",
      "Epoch:808, accuracy: 0.9000, Loss: 0.6760956401603406\n",
      "Epoch:809, accuracy: 0.9000, Loss: 0.6760940202225353\n",
      "Epoch:810, accuracy: 0.9000, Loss: 0.6760924035840297\n",
      "Epoch:811, accuracy: 0.9000, Loss: 0.6760907902342048\n",
      "Epoch:812, accuracy: 0.9000, Loss: 0.676089180162488\n",
      "Epoch:813, accuracy: 0.9000, Loss: 0.6760875733583539\n",
      "Epoch:814, accuracy: 0.9000, Loss: 0.676085969811324\n",
      "Epoch:815, accuracy: 0.9000, Loss: 0.676084369510966\n",
      "Epoch:816, accuracy: 0.9000, Loss: 0.6760827724468935\n",
      "Epoch:817, accuracy: 0.9000, Loss: 0.6760811786087665\n",
      "Epoch:818, accuracy: 0.9000, Loss: 0.6760795879862904\n",
      "Epoch:819, accuracy: 0.9000, Loss: 0.676078000569216\n",
      "Epoch:820, accuracy: 0.9000, Loss: 0.676076416347339\n",
      "Epoch:821, accuracy: 0.9000, Loss: 0.6760748353105003\n",
      "Epoch:822, accuracy: 0.9000, Loss: 0.676073257448585\n",
      "Epoch:823, accuracy: 0.9000, Loss: 0.676071682751523\n",
      "Epoch:824, accuracy: 0.9000, Loss: 0.6760701112092884\n",
      "Epoch:825, accuracy: 0.9000, Loss: 0.6760685428118983\n",
      "Epoch:826, accuracy: 0.9000, Loss: 0.6760669775494145\n",
      "Epoch:827, accuracy: 0.9000, Loss: 0.6760654154119414\n",
      "Epoch:828, accuracy: 0.9000, Loss: 0.6760638563896271\n",
      "Epoch:829, accuracy: 0.9000, Loss: 0.676062300472662\n",
      "Epoch:830, accuracy: 0.9000, Loss: 0.67606074765128\n",
      "Epoch:831, accuracy: 0.9000, Loss: 0.6760591979157564\n",
      "Epoch:832, accuracy: 0.9000, Loss: 0.6760576512564092\n",
      "Epoch:833, accuracy: 0.9000, Loss: 0.6760561076635988\n",
      "Epoch:834, accuracy: 0.9000, Loss: 0.6760545671277265\n",
      "Epoch:835, accuracy: 0.9000, Loss: 0.6760530296392355\n",
      "Epoch:836, accuracy: 0.9000, Loss: 0.6760514951886104\n",
      "Epoch:837, accuracy: 0.9000, Loss: 0.6760499637663765\n",
      "Epoch:838, accuracy: 0.9000, Loss: 0.6760484353630999\n",
      "Epoch:839, accuracy: 0.9000, Loss: 0.676046909969388\n",
      "Epoch:840, accuracy: 0.9000, Loss: 0.6760453875758874\n",
      "Epoch:841, accuracy: 0.9000, Loss: 0.6760438681732858\n",
      "Epoch:842, accuracy: 0.9000, Loss: 0.6760423517523104\n",
      "Epoch:843, accuracy: 0.9000, Loss: 0.6760408383037281\n",
      "Epoch:844, accuracy: 0.9000, Loss: 0.6760393278183454\n",
      "Epoch:845, accuracy: 0.9000, Loss: 0.676037820287008\n",
      "Epoch:846, accuracy: 0.9000, Loss: 0.6760363157006007\n",
      "Epoch:847, accuracy: 0.9000, Loss: 0.6760348140500472\n",
      "Epoch:848, accuracy: 0.9000, Loss: 0.6760333153263096\n",
      "Epoch:849, accuracy: 0.9000, Loss: 0.6760318195203887\n",
      "Epoch:850, accuracy: 0.9000, Loss: 0.6760303266233233\n",
      "Epoch:851, accuracy: 0.9000, Loss: 0.6760288366261904\n",
      "Epoch:852, accuracy: 0.9000, Loss: 0.6760273495201043\n",
      "Epoch:853, accuracy: 0.9000, Loss: 0.6760258652962179\n",
      "Epoch:854, accuracy: 0.9000, Loss: 0.6760243839457204\n",
      "Epoch:855, accuracy: 0.9000, Loss: 0.6760229054598389\n",
      "Epoch:856, accuracy: 0.9000, Loss: 0.6760214298298372\n",
      "Epoch:857, accuracy: 0.9000, Loss: 0.6760199570470158\n",
      "Epoch:858, accuracy: 0.9000, Loss: 0.676018487102712\n",
      "Epoch:859, accuracy: 0.9000, Loss: 0.6760170199882994\n",
      "Epoch:860, accuracy: 0.9000, Loss: 0.676015555695188\n",
      "Epoch:861, accuracy: 0.9000, Loss: 0.6760140942148236\n",
      "Epoch:862, accuracy: 0.9000, Loss: 0.6760126355386876\n",
      "Epoch:863, accuracy: 0.9000, Loss: 0.6760111796582976\n",
      "Epoch:864, accuracy: 0.9000, Loss: 0.676009726565206\n",
      "Epoch:865, accuracy: 0.9000, Loss: 0.6760082762510009\n",
      "Epoch:866, accuracy: 0.9000, Loss: 0.6760068287073053\n",
      "Epoch:867, accuracy: 0.9000, Loss: 0.6760053839257771\n",
      "Epoch:868, accuracy: 0.9000, Loss: 0.6760039418981089\n",
      "Epoch:869, accuracy: 0.9000, Loss: 0.6760025026160275\n",
      "Epoch:870, accuracy: 0.9000, Loss: 0.6760010660712943\n",
      "Epoch:871, accuracy: 0.9000, Loss: 0.675999632255705\n",
      "Epoch:872, accuracy: 0.9000, Loss: 0.6759982011610891\n",
      "Epoch:873, accuracy: 0.9000, Loss: 0.6759967727793096\n",
      "Epoch:874, accuracy: 0.9000, Loss: 0.6759953471022635\n",
      "Epoch:875, accuracy: 0.9000, Loss: 0.6759939241218808\n",
      "Epoch:876, accuracy: 0.9000, Loss: 0.675992503830125\n",
      "Epoch:877, accuracy: 0.9000, Loss: 0.6759910862189928\n",
      "Epoch:878, accuracy: 0.9000, Loss: 0.6759896712805139\n",
      "Epoch:879, accuracy: 0.9000, Loss: 0.6759882590067499\n",
      "Epoch:880, accuracy: 0.9000, Loss: 0.6759868493897959\n",
      "Epoch:881, accuracy: 0.9000, Loss: 0.6759854424217788\n",
      "Epoch:882, accuracy: 0.9000, Loss: 0.6759840380948581\n",
      "Epoch:883, accuracy: 0.9000, Loss: 0.6759826364012247\n",
      "Epoch:884, accuracy: 0.9000, Loss: 0.6759812373331024\n",
      "Epoch:885, accuracy: 0.9000, Loss: 0.6759798408827459\n",
      "Epoch:886, accuracy: 0.9000, Loss: 0.6759784470424415\n",
      "Epoch:887, accuracy: 0.9000, Loss: 0.6759770558045071\n",
      "Epoch:888, accuracy: 0.9000, Loss: 0.6759756671612916\n",
      "Epoch:889, accuracy: 0.9000, Loss: 0.6759742811051754\n",
      "Epoch:890, accuracy: 0.9000, Loss: 0.6759728976285695\n",
      "Epoch:891, accuracy: 0.9000, Loss: 0.6759715167239155\n",
      "Epoch:892, accuracy: 0.9000, Loss: 0.6759701383836856\n",
      "Epoch:893, accuracy: 0.9000, Loss: 0.6759687626003827\n",
      "Epoch:894, accuracy: 0.9000, Loss: 0.6759673893665399\n",
      "Epoch:895, accuracy: 0.9000, Loss: 0.6759660186747196\n",
      "Epoch:896, accuracy: 0.9000, Loss: 0.6759646505175158\n",
      "Epoch:897, accuracy: 0.9000, Loss: 0.6759632848875505\n",
      "Epoch:898, accuracy: 0.9000, Loss: 0.6759619217774765\n",
      "Epoch:899, accuracy: 0.9000, Loss: 0.6759605611799758\n",
      "Epoch:900, accuracy: 0.9000, Loss: 0.6759592030877595\n",
      "Epoch:901, accuracy: 0.9000, Loss: 0.6759578474935681\n",
      "Epoch:902, accuracy: 0.9000, Loss: 0.6759564943901714\n",
      "Epoch:903, accuracy: 0.9000, Loss: 0.6759551437703671\n",
      "Epoch:904, accuracy: 0.9000, Loss: 0.675953795626983\n",
      "Epoch:905, accuracy: 0.9000, Loss: 0.6759524499528744\n",
      "Epoch:906, accuracy: 0.9000, Loss: 0.6759511067409256\n",
      "Epoch:907, accuracy: 0.9000, Loss: 0.6759497659840494\n",
      "Epoch:908, accuracy: 0.9000, Loss: 0.6759484276751858\n",
      "Epoch:909, accuracy: 0.9000, Loss: 0.6759470918073037\n",
      "Epoch:910, accuracy: 0.9000, Loss: 0.6759457583733999\n",
      "Epoch:911, accuracy: 0.9000, Loss: 0.6759444273664983\n",
      "Epoch:912, accuracy: 0.9000, Loss: 0.6759430987796508\n",
      "Epoch:913, accuracy: 0.9000, Loss: 0.6759417726059369\n",
      "Epoch:914, accuracy: 0.9000, Loss: 0.6759404488384629\n",
      "Epoch:915, accuracy: 0.9000, Loss: 0.6759391274703628\n",
      "Epoch:916, accuracy: 0.9000, Loss: 0.6759378084947975\n",
      "Epoch:917, accuracy: 0.9000, Loss: 0.6759364919049543\n",
      "Epoch:918, accuracy: 0.9000, Loss: 0.6759351776940482\n",
      "Epoch:919, accuracy: 0.9000, Loss: 0.6759338658553201\n",
      "Epoch:920, accuracy: 0.9000, Loss: 0.6759325563820375\n",
      "Epoch:921, accuracy: 0.9000, Loss: 0.6759312492674948\n",
      "Epoch:922, accuracy: 0.9000, Loss: 0.6759299445050118\n",
      "Epoch:923, accuracy: 0.9000, Loss: 0.675928642087935\n",
      "Epoch:924, accuracy: 0.9000, Loss: 0.6759273420096368\n",
      "Epoch:925, accuracy: 0.9000, Loss: 0.6759260442635152\n",
      "Epoch:926, accuracy: 0.9000, Loss: 0.6759247488429942\n",
      "Epoch:927, accuracy: 0.9000, Loss: 0.6759234557415231\n",
      "Epoch:928, accuracy: 0.9000, Loss: 0.6759221649525768\n",
      "Epoch:929, accuracy: 0.9000, Loss: 0.6759208764696558\n",
      "Epoch:930, accuracy: 0.9000, Loss: 0.6759195902862853\n",
      "Epoch:931, accuracy: 0.9000, Loss: 0.6759183063960159\n",
      "Epoch:932, accuracy: 0.9000, Loss: 0.6759170247924232\n",
      "Epoch:933, accuracy: 0.9000, Loss: 0.6759157454691074\n",
      "Epoch:934, accuracy: 0.9000, Loss: 0.6759144684196936\n",
      "Epoch:935, accuracy: 0.9000, Loss: 0.6759131936378315\n",
      "Epoch:936, accuracy: 0.9000, Loss: 0.6759119211171951\n",
      "Epoch:937, accuracy: 0.9000, Loss: 0.6759106508514827\n",
      "Epoch:938, accuracy: 0.9000, Loss: 0.6759093828344171\n",
      "Epoch:939, accuracy: 0.9000, Loss: 0.6759081170597454\n",
      "Epoch:940, accuracy: 0.9000, Loss: 0.6759068535212379\n",
      "Epoch:941, accuracy: 0.9000, Loss: 0.6759055922126895\n",
      "Epoch:942, accuracy: 0.9000, Loss: 0.6759043331279185\n",
      "Epoch:943, accuracy: 0.9000, Loss: 0.6759030762607671\n",
      "Epoch:944, accuracy: 0.9000, Loss: 0.6759018216051007\n",
      "Epoch:945, accuracy: 0.9000, Loss: 0.6759005691548082\n",
      "Epoch:946, accuracy: 0.9000, Loss: 0.6758993189038021\n",
      "Epoch:947, accuracy: 0.9000, Loss: 0.6758980708460176\n",
      "Epoch:948, accuracy: 0.9000, Loss: 0.6758968249754135\n",
      "Epoch:949, accuracy: 0.9000, Loss: 0.6758955812859712\n",
      "Epoch:950, accuracy: 0.9000, Loss: 0.6758943397716949\n",
      "Epoch:951, accuracy: 0.9000, Loss: 0.6758931004266115\n",
      "Epoch:952, accuracy: 0.9000, Loss: 0.6758918632447712\n",
      "Epoch:953, accuracy: 0.9000, Loss: 0.6758906282202457\n",
      "Epoch:954, accuracy: 0.9000, Loss: 0.6758893953471299\n",
      "Epoch:955, accuracy: 0.9000, Loss: 0.6758881646195405\n",
      "Epoch:956, accuracy: 0.9000, Loss: 0.6758869360316168\n",
      "Epoch:957, accuracy: 0.9000, Loss: 0.6758857095775196\n",
      "Epoch:958, accuracy: 0.9000, Loss: 0.6758844852514324\n",
      "Epoch:959, accuracy: 0.9000, Loss: 0.6758832630475601\n",
      "Epoch:960, accuracy: 0.9000, Loss: 0.6758820429601294\n",
      "Epoch:961, accuracy: 0.9000, Loss: 0.6758808249833888\n",
      "Epoch:962, accuracy: 0.9000, Loss: 0.6758796091116082\n",
      "Epoch:963, accuracy: 0.9000, Loss: 0.675878395339079\n",
      "Epoch:964, accuracy: 0.9000, Loss: 0.6758771836601144\n",
      "Epoch:965, accuracy: 0.9000, Loss: 0.6758759740690481\n",
      "Epoch:966, accuracy: 0.9000, Loss: 0.6758747665602354\n",
      "Epoch:967, accuracy: 0.9000, Loss: 0.6758735611280523\n",
      "Epoch:968, accuracy: 0.9000, Loss: 0.6758723577668962\n",
      "Epoch:969, accuracy: 0.9000, Loss: 0.675871156471185\n",
      "Epoch:970, accuracy: 0.9000, Loss: 0.6758699572353577\n",
      "Epoch:971, accuracy: 0.9000, Loss: 0.6758687600538735\n",
      "Epoch:972, accuracy: 0.9000, Loss: 0.6758675649212125\n",
      "Epoch:973, accuracy: 0.9000, Loss: 0.675866371831875\n",
      "Epoch:974, accuracy: 0.9000, Loss: 0.6758651807803819\n",
      "Epoch:975, accuracy: 0.9000, Loss: 0.6758639917612741\n",
      "Epoch:976, accuracy: 0.9000, Loss: 0.6758628047691129\n",
      "Epoch:977, accuracy: 0.9000, Loss: 0.6758616197984793\n",
      "Epoch:978, accuracy: 0.9000, Loss: 0.6758604368439751\n",
      "Epoch:979, accuracy: 0.9000, Loss: 0.6758592559002208\n",
      "Epoch:980, accuracy: 0.9000, Loss: 0.6758580769618577\n",
      "Epoch:981, accuracy: 0.9000, Loss: 0.6758569000235463\n",
      "Epoch:982, accuracy: 0.9000, Loss: 0.6758557250799667\n",
      "Epoch:983, accuracy: 0.9000, Loss: 0.6758545521258188\n",
      "Epoch:984, accuracy: 0.9000, Loss: 0.6758533811558213\n",
      "Epoch:985, accuracy: 0.9000, Loss: 0.6758522121647131\n",
      "Epoch:986, accuracy: 0.9000, Loss: 0.6758510451472517\n",
      "Epoch:987, accuracy: 0.9000, Loss: 0.6758498800982139\n",
      "Epoch:988, accuracy: 0.9000, Loss: 0.6758487170123955\n",
      "Epoch:989, accuracy: 0.9000, Loss: 0.6758475558846114\n",
      "Epoch:990, accuracy: 0.9000, Loss: 0.6758463967096955\n",
      "Epoch:991, accuracy: 0.9000, Loss: 0.6758452394825\n",
      "Epoch:992, accuracy: 0.9000, Loss: 0.6758440841978962\n",
      "Epoch:993, accuracy: 0.9000, Loss: 0.6758429308507742\n",
      "Epoch:994, accuracy: 0.9000, Loss: 0.6758417794360418\n",
      "Epoch:995, accuracy: 0.9000, Loss: 0.6758406299486264\n",
      "Epoch:996, accuracy: 0.9000, Loss: 0.6758394823834729\n",
      "Epoch:997, accuracy: 0.9000, Loss: 0.6758383367355447\n",
      "Epoch:998, accuracy: 0.9000, Loss: 0.6758371929998235\n",
      "Epoch:999, accuracy: 0.9000, Loss: 0.6758360511713087\n"
     ]
    }
   ],
   "source": [
    "# Create RNN layer\n",
    "rnn_layer = Layer_RNN(n_inputs=3, n_neurons=5, n_outputs=2)\n",
    "\n",
    "# Activation & Loss\n",
    "activation_softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.01, decay=1e-3)\n",
    "\n",
    "# Dummy data\n",
    "X = np.random.randn(10, 3)  # 10 samples, 3 features\n",
    "y = np.random.randint(0, 2, size=(10,))  # 10 target labels (0 or 1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    rnn_layer.forward(X)\n",
    "    loss = activation_softmax_loss.forward(rnn_layer.output, y)\n",
    "\n",
    "    # Get predictions (class with the highest probability)\n",
    "    predictions = np.argmax(activation_softmax_loss.output, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # Backward pass\n",
    "    activation_softmax_loss.backward(activation_softmax_loss.output, y)\n",
    "    rnn_layer.backward(activation_softmax_loss.dinputs)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(rnn_layer)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    print(f\"Epoch:{epoch}, accuracy: {accuracy:.4f}, Loss: {loss.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62614319,  0.75376038, -0.03483344])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "rnn_layer.forward(np.array([0.7, 0.7, -0.2]))\n",
    "loss = activation_softmax_loss.forward(rnn_layer.output, y)\n",
    "\n",
    "# Get predictions (class with the highest probability)\n",
    "predictions = np.argmax(activation_softmax_loss.output, axis=1)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
