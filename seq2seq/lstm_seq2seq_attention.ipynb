{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanjana Rayarala\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.lstm(x)  \n",
    "        return outputs, h, c  \n",
    "\n",
    "#Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  \n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)  \n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.repeat(1, seq_len, 1)  \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  \n",
    "        attn_weights = torch.softmax(self.v(energy), dim=1)  \n",
    "        context = torch.sum(attn_weights * encoder_outputs, dim=1)  \n",
    "        return context, attn_weights  \n",
    "\n",
    "#Decoder LSTM with Attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        context, attn_weights = self.attention(hidden[-1].unsqueeze(1), encoder_outputs)  \n",
    "        lstm_input = torch.cat((x, context.unsqueeze(1)), dim=2)  \n",
    "        output, (h, c) = self.lstm(lstm_input, (hidden, cell))  \n",
    "        output = self.fc(output.squeeze(1))  \n",
    "        return output, h, c, attn_weights  \n",
    "\n",
    "#Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, target_seq_len, teacher_forcing_ratio=0.5):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        encoder_outputs, h, c = self.encoder(x)  \n",
    "        decoder_input = x[:, -1, :].unsqueeze(1)  \n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(target_seq_len):\n",
    "            output, h, c, _ = self.decoder(decoder_input, h, c, encoder_outputs)\n",
    "            outputs.append(output)\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                decoder_input = output.unsqueeze(1)  \n",
    "            else:\n",
    "                decoder_input = decoder_input  \n",
    "        return torch.stack(outputs, dim=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Function\n",
    "def create_sequences(data, seq_length, target_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - target_length + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length:i + seq_length + target_length])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example Data (Stock Prices)\n",
    "stock_prices = np.array([100, 102, 105, 107, 110, 115, 120, 125, 130, 128, 132, 135], dtype=np.float32)\n",
    "\n",
    "# Create Dataset\n",
    "seq_length = 4  \n",
    "target_length = 2  \n",
    "X, y = create_sequences(stock_prices, seq_length, target_length)\n",
    "X_train = torch.tensor(X).unsqueeze(-1)  \n",
    "y_train = torch.tensor(y).unsqueeze(-1)  \n",
    "\n",
    "# Model Setup\n",
    "input_size = 1  \n",
    "hidden_size = 64  \n",
    "num_layers = 2  \n",
    "output_size = 1  \n",
    "\n",
    "model = Seq2Seq(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Function\n",
    "def train(model, X_train, y_train, epochs=100):\n",
    "    model.train()  \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train, target_seq_len=target_length, teacher_forcing_ratio=0.5)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate(model, X_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        pred = model(X_input, target_seq_len=target_length, teacher_forcing_ratio=0.0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 50.5362\n",
      "Epoch [20/5000], Loss: 50.5357\n",
      "Epoch [30/5000], Loss: 50.5354\n",
      "Epoch [40/5000], Loss: 50.5350\n",
      "Epoch [50/5000], Loss: 50.5346\n",
      "Epoch [60/5000], Loss: 50.5342\n",
      "Epoch [70/5000], Loss: 50.5338\n",
      "Epoch [80/5000], Loss: 50.5334\n",
      "Epoch [90/5000], Loss: 50.5331\n",
      "Epoch [100/5000], Loss: 50.5326\n",
      "Epoch [110/5000], Loss: 50.5322\n",
      "Epoch [120/5000], Loss: 50.5318\n",
      "Epoch [130/5000], Loss: 50.5314\n",
      "Epoch [140/5000], Loss: 50.5309\n",
      "Epoch [150/5000], Loss: 50.5305\n",
      "Epoch [160/5000], Loss: 50.5300\n",
      "Epoch [170/5000], Loss: 50.5297\n",
      "Epoch [180/5000], Loss: 50.5292\n",
      "Epoch [190/5000], Loss: 50.5288\n",
      "Epoch [200/5000], Loss: 50.5284\n",
      "Epoch [210/5000], Loss: 50.5279\n",
      "Epoch [220/5000], Loss: 50.5275\n",
      "Epoch [230/5000], Loss: 50.5270\n",
      "Epoch [240/5000], Loss: 50.5266\n",
      "Epoch [250/5000], Loss: 50.5262\n",
      "Epoch [260/5000], Loss: 50.5257\n",
      "Epoch [270/5000], Loss: 50.5253\n",
      "Epoch [280/5000], Loss: 50.5247\n",
      "Epoch [290/5000], Loss: 50.5243\n",
      "Epoch [300/5000], Loss: 50.5238\n",
      "Epoch [310/5000], Loss: 50.5234\n",
      "Epoch [320/5000], Loss: 50.5229\n",
      "Epoch [330/5000], Loss: 50.5225\n",
      "Epoch [340/5000], Loss: 50.5220\n",
      "Epoch [350/5000], Loss: 50.5215\n",
      "Epoch [360/5000], Loss: 50.5210\n",
      "Epoch [370/5000], Loss: 50.5205\n",
      "Epoch [380/5000], Loss: 50.5200\n",
      "Epoch [390/5000], Loss: 50.5195\n",
      "Epoch [400/5000], Loss: 50.5191\n",
      "Epoch [410/5000], Loss: 50.5186\n",
      "Epoch [420/5000], Loss: 50.5180\n",
      "Epoch [430/5000], Loss: 50.5175\n",
      "Epoch [440/5000], Loss: 50.5171\n",
      "Epoch [450/5000], Loss: 50.5165\n",
      "Epoch [460/5000], Loss: 50.5160\n",
      "Epoch [470/5000], Loss: 50.5155\n",
      "Epoch [480/5000], Loss: 50.5150\n",
      "Epoch [490/5000], Loss: 50.5145\n",
      "Epoch [500/5000], Loss: 50.5140\n",
      "Epoch [510/5000], Loss: 50.5134\n",
      "Epoch [520/5000], Loss: 50.5129\n",
      "Epoch [530/5000], Loss: 50.5124\n",
      "Epoch [540/5000], Loss: 50.5119\n",
      "Epoch [550/5000], Loss: 50.5114\n",
      "Epoch [560/5000], Loss: 50.5108\n",
      "Epoch [570/5000], Loss: 50.5103\n",
      "Epoch [580/5000], Loss: 50.5097\n",
      "Epoch [590/5000], Loss: 50.5091\n",
      "Epoch [600/5000], Loss: 50.5086\n",
      "Epoch [610/5000], Loss: 50.5081\n",
      "Epoch [620/5000], Loss: 50.5075\n",
      "Epoch [630/5000], Loss: 50.5069\n",
      "Epoch [640/5000], Loss: 50.5064\n",
      "Epoch [650/5000], Loss: 50.5058\n",
      "Epoch [660/5000], Loss: 50.5052\n",
      "Epoch [670/5000], Loss: 50.5047\n",
      "Epoch [680/5000], Loss: 50.5041\n",
      "Epoch [690/5000], Loss: 50.5035\n",
      "Epoch [700/5000], Loss: 50.5029\n",
      "Epoch [710/5000], Loss: 50.5024\n",
      "Epoch [720/5000], Loss: 50.5018\n",
      "Epoch [730/5000], Loss: 50.5012\n",
      "Epoch [740/5000], Loss: 50.5006\n",
      "Epoch [750/5000], Loss: 50.5000\n",
      "Epoch [760/5000], Loss: 50.4994\n",
      "Epoch [770/5000], Loss: 50.4988\n",
      "Epoch [780/5000], Loss: 50.4981\n",
      "Epoch [790/5000], Loss: 50.4975\n",
      "Epoch [800/5000], Loss: 50.4970\n",
      "Epoch [810/5000], Loss: 50.4963\n",
      "Epoch [820/5000], Loss: 50.4957\n",
      "Epoch [830/5000], Loss: 50.4951\n",
      "Epoch [840/5000], Loss: 50.4945\n",
      "Epoch [850/5000], Loss: 50.4939\n",
      "Epoch [860/5000], Loss: 50.4932\n",
      "Epoch [870/5000], Loss: 50.4925\n",
      "Epoch [880/5000], Loss: 50.4919\n",
      "Epoch [890/5000], Loss: 50.4913\n",
      "Epoch [900/5000], Loss: 50.4907\n",
      "Epoch [910/5000], Loss: 50.4900\n",
      "Epoch [920/5000], Loss: 50.4894\n",
      "Epoch [930/5000], Loss: 50.4887\n",
      "Epoch [940/5000], Loss: 50.4880\n",
      "Epoch [950/5000], Loss: 50.4874\n",
      "Epoch [960/5000], Loss: 50.4867\n",
      "Epoch [970/5000], Loss: 50.4860\n",
      "Epoch [980/5000], Loss: 50.4854\n",
      "Epoch [990/5000], Loss: 50.4847\n",
      "Epoch [1000/5000], Loss: 50.4840\n",
      "Epoch [1010/5000], Loss: 50.4833\n",
      "Epoch [1020/5000], Loss: 50.4826\n",
      "Epoch [1030/5000], Loss: 50.4819\n",
      "Epoch [1040/5000], Loss: 50.4812\n",
      "Epoch [1050/5000], Loss: 50.4805\n",
      "Epoch [1060/5000], Loss: 50.4798\n",
      "Epoch [1070/5000], Loss: 50.4791\n",
      "Epoch [1080/5000], Loss: 50.4784\n",
      "Epoch [1090/5000], Loss: 50.4776\n",
      "Epoch [1100/5000], Loss: 50.4768\n",
      "Epoch [1110/5000], Loss: 50.4760\n",
      "Epoch [1120/5000], Loss: 50.4749\n",
      "Epoch [1130/5000], Loss: 50.4740\n",
      "Epoch [1140/5000], Loss: 50.4731\n",
      "Epoch [1150/5000], Loss: 50.4721\n",
      "Epoch [1160/5000], Loss: 50.4709\n",
      "Epoch [1170/5000], Loss: 50.4697\n",
      "Epoch [1180/5000], Loss: 50.4680\n",
      "Epoch [1190/5000], Loss: 50.4287\n",
      "Epoch [1200/5000], Loss: 50.1878\n",
      "Epoch [1210/5000], Loss: 50.1525\n",
      "Epoch [1220/5000], Loss: 50.1370\n",
      "Epoch [1230/5000], Loss: 50.1338\n",
      "Epoch [1240/5000], Loss: 50.1333\n",
      "Epoch [1250/5000], Loss: 50.1328\n",
      "Epoch [1260/5000], Loss: 50.1320\n",
      "Epoch [1270/5000], Loss: 50.1312\n",
      "Epoch [1280/5000], Loss: 50.1302\n",
      "Epoch [1290/5000], Loss: 50.1285\n",
      "Epoch [1300/5000], Loss: 50.1276\n",
      "Epoch [1310/5000], Loss: 50.1160\n",
      "Epoch [1320/5000], Loss: 49.8963\n",
      "Epoch [1330/5000], Loss: 49.8059\n",
      "Epoch [1340/5000], Loss: 49.8759\n",
      "Epoch [1350/5000], Loss: 49.9571\n",
      "Epoch [1360/5000], Loss: 49.9288\n",
      "Epoch [1370/5000], Loss: 49.8893\n",
      "Epoch [1380/5000], Loss: 49.7529\n",
      "Epoch [1390/5000], Loss: 49.7290\n",
      "Epoch [1400/5000], Loss: 49.6867\n",
      "Epoch [1410/5000], Loss: 49.5656\n",
      "Epoch [1420/5000], Loss: 49.3461\n",
      "Epoch [1430/5000], Loss: 48.9209\n",
      "Epoch [1440/5000], Loss: 49.0673\n",
      "Epoch [1450/5000], Loss: 48.6061\n",
      "Epoch [1460/5000], Loss: 48.0848\n",
      "Epoch [1470/5000], Loss: 48.8823\n",
      "Epoch [1480/5000], Loss: 48.3943\n",
      "Epoch [1490/5000], Loss: 47.8207\n",
      "Epoch [1500/5000], Loss: 47.1806\n",
      "Epoch [1510/5000], Loss: 46.8334\n",
      "Epoch [1520/5000], Loss: 45.8535\n",
      "Epoch [1530/5000], Loss: 38.2766\n",
      "Epoch [1540/5000], Loss: 32.8470\n",
      "Epoch [1550/5000], Loss: 30.4324\n",
      "Epoch [1560/5000], Loss: 25.8155\n",
      "Epoch [1570/5000], Loss: 23.3965\n",
      "Epoch [1580/5000], Loss: 26.8454\n",
      "Epoch [1590/5000], Loss: 22.7659\n",
      "Epoch [1600/5000], Loss: 21.5133\n",
      "Epoch [1610/5000], Loss: 20.5466\n",
      "Epoch [1620/5000], Loss: 20.5504\n",
      "Epoch [1630/5000], Loss: 17.9592\n",
      "Epoch [1640/5000], Loss: 13.8950\n",
      "Epoch [1650/5000], Loss: 43.7009\n",
      "Epoch [1660/5000], Loss: 17.0758\n",
      "Epoch [1670/5000], Loss: 13.5830\n",
      "Epoch [1680/5000], Loss: 9.4599\n",
      "Epoch [1690/5000], Loss: 7.9935\n",
      "Epoch [1700/5000], Loss: 6.0622\n",
      "Epoch [1710/5000], Loss: 5.0374\n",
      "Epoch [1720/5000], Loss: 4.7824\n",
      "Epoch [1730/5000], Loss: 4.1418\n",
      "Epoch [1740/5000], Loss: 4.1970\n",
      "Epoch [1750/5000], Loss: 3.3073\n",
      "Epoch [1760/5000], Loss: 2.9869\n",
      "Epoch [1770/5000], Loss: 2.7684\n",
      "Epoch [1780/5000], Loss: 3.4590\n",
      "Epoch [1790/5000], Loss: 3.7329\n",
      "Epoch [1800/5000], Loss: 2.6054\n",
      "Epoch [1810/5000], Loss: 2.5740\n",
      "Epoch [1820/5000], Loss: 2.3795\n",
      "Epoch [1830/5000], Loss: 2.2760\n",
      "Epoch [1840/5000], Loss: 8.5165\n",
      "Epoch [1850/5000], Loss: 3.1820\n",
      "Epoch [1860/5000], Loss: 2.4748\n",
      "Epoch [1870/5000], Loss: 2.4609\n",
      "Epoch [1880/5000], Loss: 2.0643\n",
      "Epoch [1890/5000], Loss: 1.9778\n",
      "Epoch [1900/5000], Loss: 1.9159\n",
      "Epoch [1910/5000], Loss: 3.0718\n",
      "Epoch [1920/5000], Loss: 2.7031\n",
      "Epoch [1930/5000], Loss: 2.3484\n",
      "Epoch [1940/5000], Loss: 2.0024\n",
      "Epoch [1950/5000], Loss: 1.7053\n",
      "Epoch [1960/5000], Loss: 1.4338\n",
      "Epoch [1970/5000], Loss: 1.3307\n",
      "Epoch [1980/5000], Loss: 1.2484\n",
      "Epoch [1990/5000], Loss: 1.1690\n",
      "Epoch [2000/5000], Loss: 1.1134\n",
      "Epoch [2010/5000], Loss: 1.0202\n",
      "Epoch [2020/5000], Loss: 0.9730\n",
      "Epoch [2030/5000], Loss: 0.9097\n",
      "Epoch [2040/5000], Loss: 0.9279\n",
      "Epoch [2050/5000], Loss: 0.7984\n",
      "Epoch [2060/5000], Loss: 0.7433\n",
      "Epoch [2070/5000], Loss: 0.7118\n",
      "Epoch [2080/5000], Loss: 0.6772\n",
      "Epoch [2090/5000], Loss: 0.6545\n",
      "Epoch [2100/5000], Loss: 0.6347\n",
      "Epoch [2110/5000], Loss: 0.6180\n",
      "Epoch [2120/5000], Loss: 0.6033\n",
      "Epoch [2130/5000], Loss: 0.5902\n",
      "Epoch [2140/5000], Loss: 0.7097\n",
      "Epoch [2150/5000], Loss: 0.5980\n",
      "Epoch [2160/5000], Loss: 0.6133\n",
      "Epoch [2170/5000], Loss: 0.5612\n",
      "Epoch [2180/5000], Loss: 0.5491\n",
      "Epoch [2190/5000], Loss: 0.5421\n",
      "Epoch [2200/5000], Loss: 0.5337\n",
      "Epoch [2210/5000], Loss: 0.5279\n",
      "Epoch [2220/5000], Loss: 0.5229\n",
      "Epoch [2230/5000], Loss: 0.5203\n",
      "Epoch [2240/5000], Loss: 0.5330\n",
      "Epoch [2250/5000], Loss: 0.5096\n",
      "Epoch [2260/5000], Loss: 0.5169\n",
      "Epoch [2270/5000], Loss: 0.5233\n",
      "Epoch [2280/5000], Loss: 0.5108\n",
      "Epoch [2290/5000], Loss: 0.4936\n",
      "Epoch [2300/5000], Loss: 0.5396\n",
      "Epoch [2310/5000], Loss: 0.4848\n",
      "Epoch [2320/5000], Loss: 0.4818\n",
      "Epoch [2330/5000], Loss: 0.4943\n",
      "Epoch [2340/5000], Loss: 0.4847\n",
      "Epoch [2350/5000], Loss: 0.5017\n",
      "Epoch [2360/5000], Loss: 0.4646\n",
      "Epoch [2370/5000], Loss: 0.5080\n",
      "Epoch [2380/5000], Loss: 0.4557\n",
      "Epoch [2390/5000], Loss: 0.5007\n",
      "Epoch [2400/5000], Loss: 0.4637\n",
      "Epoch [2410/5000], Loss: 0.4855\n",
      "Epoch [2420/5000], Loss: 0.4424\n",
      "Epoch [2430/5000], Loss: 0.4346\n",
      "Epoch [2440/5000], Loss: 0.4296\n",
      "Epoch [2450/5000], Loss: 0.4284\n",
      "Epoch [2460/5000], Loss: 0.4227\n",
      "Epoch [2470/5000], Loss: 0.4455\n",
      "Epoch [2480/5000], Loss: 0.4193\n",
      "Epoch [2490/5000], Loss: 0.4098\n",
      "Epoch [2500/5000], Loss: 0.4023\n",
      "Epoch [2510/5000], Loss: 0.3976\n",
      "Epoch [2520/5000], Loss: 0.4035\n",
      "Epoch [2530/5000], Loss: 0.3913\n",
      "Epoch [2540/5000], Loss: 0.3926\n",
      "Epoch [2550/5000], Loss: 0.3882\n",
      "Epoch [2560/5000], Loss: 0.3803\n",
      "Epoch [2570/5000], Loss: 0.4236\n",
      "Epoch [2580/5000], Loss: 0.4091\n",
      "Epoch [2590/5000], Loss: 0.3840\n",
      "Epoch [2600/5000], Loss: 0.3696\n",
      "Epoch [2610/5000], Loss: 0.3680\n",
      "Epoch [2620/5000], Loss: 0.3703\n",
      "Epoch [2630/5000], Loss: 0.3786\n",
      "Epoch [2640/5000], Loss: 0.3597\n",
      "Epoch [2650/5000], Loss: 0.3775\n",
      "Epoch [2660/5000], Loss: 0.3571\n",
      "Epoch [2670/5000], Loss: 0.3722\n",
      "Epoch [2680/5000], Loss: 0.3551\n",
      "Epoch [2690/5000], Loss: 0.3878\n",
      "Epoch [2700/5000], Loss: 0.3642\n",
      "Epoch [2710/5000], Loss: 0.3528\n",
      "Epoch [2720/5000], Loss: 0.3515\n",
      "Epoch [2730/5000], Loss: 0.3477\n",
      "Epoch [2740/5000], Loss: 0.3939\n",
      "Epoch [2750/5000], Loss: 0.3642\n",
      "Epoch [2760/5000], Loss: 0.3501\n",
      "Epoch [2770/5000], Loss: 0.3378\n",
      "Epoch [2780/5000], Loss: 0.3372\n",
      "Epoch [2790/5000], Loss: 0.3376\n",
      "Epoch [2800/5000], Loss: 0.4109\n",
      "Epoch [2810/5000], Loss: 0.3332\n",
      "Epoch [2820/5000], Loss: 0.3490\n",
      "Epoch [2830/5000], Loss: 0.3427\n",
      "Epoch [2840/5000], Loss: 0.3388\n",
      "Epoch [2850/5000], Loss: 0.3342\n",
      "Epoch [2860/5000], Loss: 0.3374\n",
      "Epoch [2870/5000], Loss: 0.3334\n",
      "Epoch [2880/5000], Loss: 0.3395\n",
      "Epoch [2890/5000], Loss: 0.3344\n",
      "Epoch [2900/5000], Loss: 0.3428\n",
      "Epoch [2910/5000], Loss: 0.3210\n",
      "Epoch [2920/5000], Loss: 0.3437\n",
      "Epoch [2930/5000], Loss: 0.3197\n",
      "Epoch [2940/5000], Loss: 0.3219\n",
      "Epoch [2950/5000], Loss: 0.3231\n",
      "Epoch [2960/5000], Loss: 0.3360\n",
      "Epoch [2970/5000], Loss: 0.3235\n",
      "Epoch [2980/5000], Loss: 0.3335\n",
      "Epoch [2990/5000], Loss: 0.3157\n",
      "Epoch [3000/5000], Loss: 0.3325\n",
      "Epoch [3010/5000], Loss: 0.3185\n",
      "Epoch [3020/5000], Loss: 0.3383\n",
      "Epoch [3030/5000], Loss: 0.3123\n",
      "Epoch [3040/5000], Loss: 0.3256\n",
      "Epoch [3050/5000], Loss: 0.3132\n",
      "Epoch [3060/5000], Loss: 0.3087\n",
      "Epoch [3070/5000], Loss: 0.4327\n",
      "Epoch [3080/5000], Loss: 0.3541\n",
      "Epoch [3090/5000], Loss: 0.3521\n",
      "Epoch [3100/5000], Loss: 0.3129\n",
      "Epoch [3110/5000], Loss: 0.3065\n",
      "Epoch [3120/5000], Loss: 0.3058\n",
      "Epoch [3130/5000], Loss: 0.3040\n",
      "Epoch [3140/5000], Loss: 0.3031\n",
      "Epoch [3150/5000], Loss: 0.3019\n",
      "Epoch [3160/5000], Loss: 0.3599\n",
      "Epoch [3170/5000], Loss: 0.4417\n",
      "Epoch [3180/5000], Loss: 0.3332\n",
      "Epoch [3190/5000], Loss: 0.3110\n",
      "Epoch [3200/5000], Loss: 0.2991\n",
      "Epoch [3210/5000], Loss: 0.3003\n",
      "Epoch [3220/5000], Loss: 0.2978\n",
      "Epoch [3230/5000], Loss: 0.2968\n",
      "Epoch [3240/5000], Loss: 0.2956\n",
      "Epoch [3250/5000], Loss: 0.2986\n",
      "Epoch [3260/5000], Loss: 0.4610\n",
      "Epoch [3270/5000], Loss: 0.3703\n",
      "Epoch [3280/5000], Loss: 0.3025\n",
      "Epoch [3290/5000], Loss: 0.2965\n",
      "Epoch [3300/5000], Loss: 0.2936\n",
      "Epoch [3310/5000], Loss: 0.2931\n",
      "Epoch [3320/5000], Loss: 0.2918\n",
      "Epoch [3330/5000], Loss: 0.2903\n",
      "Epoch [3340/5000], Loss: 0.2897\n",
      "Epoch [3350/5000], Loss: 0.2947\n",
      "Epoch [3360/5000], Loss: 0.4298\n",
      "Epoch [3370/5000], Loss: 0.3396\n",
      "Epoch [3380/5000], Loss: 0.2952\n",
      "Epoch [3390/5000], Loss: 0.2909\n",
      "Epoch [3400/5000], Loss: 0.2942\n",
      "Epoch [3410/5000], Loss: 0.2857\n",
      "Epoch [3420/5000], Loss: 0.2769\n",
      "Epoch [3430/5000], Loss: 0.3282\n",
      "Epoch [3440/5000], Loss: 9.1556\n",
      "Epoch [3450/5000], Loss: 4.0402\n",
      "Epoch [3460/5000], Loss: 3.9336\n",
      "Epoch [3470/5000], Loss: 1.8606\n",
      "Epoch [3480/5000], Loss: 2.0195\n",
      "Epoch [3490/5000], Loss: 1.4281\n",
      "Epoch [3500/5000], Loss: 2.0461\n",
      "Epoch [3510/5000], Loss: 1.0540\n",
      "Epoch [3520/5000], Loss: 1.0739\n",
      "Epoch [3530/5000], Loss: 0.9794\n",
      "Epoch [3540/5000], Loss: 0.9135\n",
      "Epoch [3550/5000], Loss: 0.8900\n",
      "Epoch [3560/5000], Loss: 0.8821\n",
      "Epoch [3570/5000], Loss: 0.8761\n",
      "Epoch [3580/5000], Loss: 0.8712\n",
      "Epoch [3590/5000], Loss: 0.8676\n",
      "Epoch [3600/5000], Loss: 0.8646\n",
      "Epoch [3610/5000], Loss: 0.8618\n",
      "Epoch [3620/5000], Loss: 0.8593\n",
      "Epoch [3630/5000], Loss: 0.8570\n",
      "Epoch [3640/5000], Loss: 0.8565\n",
      "Epoch [3650/5000], Loss: 0.8530\n",
      "Epoch [3660/5000], Loss: 0.8514\n",
      "Epoch [3670/5000], Loss: 0.8497\n",
      "Epoch [3680/5000], Loss: 0.8472\n",
      "Epoch [3690/5000], Loss: 0.8445\n",
      "Epoch [3700/5000], Loss: 0.8420\n",
      "Epoch [3710/5000], Loss: 0.8400\n",
      "Epoch [3720/5000], Loss: 0.8384\n",
      "Epoch [3730/5000], Loss: 0.8370\n",
      "Epoch [3740/5000], Loss: 0.8357\n",
      "Epoch [3750/5000], Loss: 0.8346\n",
      "Epoch [3760/5000], Loss: 0.8336\n",
      "Epoch [3770/5000], Loss: 0.8326\n",
      "Epoch [3780/5000], Loss: 0.8317\n",
      "Epoch [3790/5000], Loss: 0.8308\n",
      "Epoch [3800/5000], Loss: 0.8299\n",
      "Epoch [3810/5000], Loss: 0.8283\n",
      "Epoch [3820/5000], Loss: 0.8118\n",
      "Epoch [3830/5000], Loss: 0.8003\n",
      "Epoch [3840/5000], Loss: 0.7897\n",
      "Epoch [3850/5000], Loss: 0.7815\n",
      "Epoch [3860/5000], Loss: 0.7747\n",
      "Epoch [3870/5000], Loss: 0.7691\n",
      "Epoch [3880/5000], Loss: 0.7643\n",
      "Epoch [3890/5000], Loss: 0.7601\n",
      "Epoch [3900/5000], Loss: 0.7564\n",
      "Epoch [3910/5000], Loss: 0.7531\n",
      "Epoch [3920/5000], Loss: 0.7501\n",
      "Epoch [3930/5000], Loss: 0.7475\n",
      "Epoch [3940/5000], Loss: 0.7450\n",
      "Epoch [3950/5000], Loss: 0.7427\n",
      "Epoch [3960/5000], Loss: 0.7406\n",
      "Epoch [3970/5000], Loss: 0.7387\n",
      "Epoch [3980/5000], Loss: 0.7369\n",
      "Epoch [3990/5000], Loss: 0.7351\n",
      "Epoch [4000/5000], Loss: 0.7333\n",
      "Epoch [4010/5000], Loss: 0.7318\n",
      "Epoch [4020/5000], Loss: 0.7306\n",
      "Epoch [4030/5000], Loss: 0.7420\n",
      "Epoch [4040/5000], Loss: 0.7282\n",
      "Epoch [4050/5000], Loss: 0.7328\n",
      "Epoch [4060/5000], Loss: 0.7328\n",
      "Epoch [4070/5000], Loss: 0.7268\n",
      "Epoch [4080/5000], Loss: 0.7460\n",
      "Epoch [4090/5000], Loss: 0.7219\n",
      "Epoch [4100/5000], Loss: 0.7251\n",
      "Epoch [4110/5000], Loss: 0.7287\n",
      "Epoch [4120/5000], Loss: 0.7298\n",
      "Epoch [4130/5000], Loss: 0.7267\n",
      "Epoch [4140/5000], Loss: 0.7197\n",
      "Epoch [4150/5000], Loss: 0.7534\n",
      "Epoch [4160/5000], Loss: 0.7169\n",
      "Epoch [4170/5000], Loss: 0.7347\n",
      "Epoch [4180/5000], Loss: 0.7139\n",
      "Epoch [4190/5000], Loss: 0.7124\n",
      "Epoch [4200/5000], Loss: 0.7759\n",
      "Epoch [4210/5000], Loss: 0.7449\n",
      "Epoch [4220/5000], Loss: 0.9690\n",
      "Epoch [4230/5000], Loss: 3.6929\n",
      "Epoch [4240/5000], Loss: 4.1482\n",
      "Epoch [4250/5000], Loss: 2.3262\n",
      "Epoch [4260/5000], Loss: 1.5147\n",
      "Epoch [4270/5000], Loss: 1.0909\n",
      "Epoch [4280/5000], Loss: 1.0024\n",
      "Epoch [4290/5000], Loss: 0.9712\n",
      "Epoch [4300/5000], Loss: 0.9404\n",
      "Epoch [4310/5000], Loss: 0.9273\n",
      "Epoch [4320/5000], Loss: 0.9144\n",
      "Epoch [4330/5000], Loss: 0.8939\n",
      "Epoch [4340/5000], Loss: 0.8722\n",
      "Epoch [4350/5000], Loss: 0.8577\n",
      "Epoch [4360/5000], Loss: 0.8460\n",
      "Epoch [4370/5000], Loss: 0.8361\n",
      "Epoch [4380/5000], Loss: 0.9015\n",
      "Epoch [4390/5000], Loss: 0.8421\n",
      "Epoch [4400/5000], Loss: 0.8303\n",
      "Epoch [4410/5000], Loss: 0.8189\n",
      "Epoch [4420/5000], Loss: 0.8091\n",
      "Epoch [4430/5000], Loss: 0.8044\n",
      "Epoch [4440/5000], Loss: 0.8004\n",
      "Epoch [4450/5000], Loss: 0.7973\n",
      "Epoch [4460/5000], Loss: 0.7945\n",
      "Epoch [4470/5000], Loss: 0.7919\n",
      "Epoch [4480/5000], Loss: 0.7895\n",
      "Epoch [4490/5000], Loss: 0.7871\n",
      "Epoch [4500/5000], Loss: 0.7850\n",
      "Epoch [4510/5000], Loss: 0.7829\n",
      "Epoch [4520/5000], Loss: 0.7809\n",
      "Epoch [4530/5000], Loss: 0.7790\n",
      "Epoch [4540/5000], Loss: 2.4296\n",
      "Epoch [4550/5000], Loss: 1.4432\n",
      "Epoch [4560/5000], Loss: 0.8676\n",
      "Epoch [4570/5000], Loss: 0.8913\n",
      "Epoch [4580/5000], Loss: 0.8464\n",
      "Epoch [4590/5000], Loss: 0.8026\n",
      "Epoch [4600/5000], Loss: 0.7795\n",
      "Epoch [4610/5000], Loss: 0.7740\n",
      "Epoch [4620/5000], Loss: 0.7697\n",
      "Epoch [4630/5000], Loss: 0.7664\n",
      "Epoch [4640/5000], Loss: 0.7640\n",
      "Epoch [4650/5000], Loss: 0.7620\n",
      "Epoch [4660/5000], Loss: 0.7601\n",
      "Epoch [4670/5000], Loss: 0.7582\n",
      "Epoch [4680/5000], Loss: 0.7562\n",
      "Epoch [4690/5000], Loss: 0.7546\n",
      "Epoch [4700/5000], Loss: 0.7902\n",
      "Epoch [4710/5000], Loss: 0.7894\n",
      "Epoch [4720/5000], Loss: 0.7618\n",
      "Epoch [4730/5000], Loss: 0.7545\n",
      "Epoch [4740/5000], Loss: 0.7511\n",
      "Epoch [4750/5000], Loss: 0.7495\n",
      "Epoch [4760/5000], Loss: 0.7480\n",
      "Epoch [4770/5000], Loss: 0.7469\n",
      "Epoch [4780/5000], Loss: 0.7459\n",
      "Epoch [4790/5000], Loss: 0.7449\n",
      "Epoch [4800/5000], Loss: 0.7440\n",
      "Epoch [4810/5000], Loss: 0.7431\n",
      "Epoch [4820/5000], Loss: 0.7423\n",
      "Epoch [4830/5000], Loss: 0.7414\n",
      "Epoch [4840/5000], Loss: 0.7406\n",
      "Epoch [4850/5000], Loss: 0.7398\n",
      "Epoch [4860/5000], Loss: 0.7390\n",
      "Epoch [4870/5000], Loss: 0.7382\n",
      "Epoch [4880/5000], Loss: 0.7375\n",
      "Epoch [4890/5000], Loss: 0.7370\n",
      "Epoch [4900/5000], Loss: 0.7688\n",
      "Epoch [4910/5000], Loss: 0.7489\n",
      "Epoch [4920/5000], Loss: 0.7347\n",
      "Epoch [4930/5000], Loss: 0.7384\n",
      "Epoch [4940/5000], Loss: 0.7327\n",
      "Epoch [4950/5000], Loss: 0.7621\n",
      "Epoch [4960/5000], Loss: 0.7345\n",
      "Epoch [4970/5000], Loss: 0.7380\n",
      "Epoch [4980/5000], Loss: 0.7332\n",
      "Epoch [4990/5000], Loss: 0.7308\n",
      "Epoch [5000/5000], Loss: 0.7290\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "train(model, X_train, y_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Stock Prices: [132.19366455078125, 134.63034057617188]\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model on New Data\n",
    "X_test = torch.tensor([[125, 130, 128, 132]], dtype=torch.float32).unsqueeze(-1)  \n",
    "predicted = evaluate(model, X_test)\n",
    "\n",
    "print(\"\\nPredicted Stock Prices:\", predicted.squeeze().tolist())  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
