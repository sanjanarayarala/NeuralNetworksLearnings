{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanjana Rayarala\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers= num_layers\n",
    "        self.lstm= nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs, (h,c)= self.lstm(inputs)\n",
    "        return h,c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size= hidden_size\n",
    "        self.num_layers= num_layers\n",
    "        self.lstm= nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc= nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden,cell)= self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(outputs.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(output_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: (batch_size, src_len, input_dim)\n",
    "        # trg: (batch_size, trg_len, output_dim)\n",
    "\n",
    "        batch_size, trg_len, _ =trg.shape\n",
    "        outputs= torch.zeros(batch_size, trg_len, trg.shape[-1]).to(src.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:,0,:].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell =self.decoder(input, hidden, cell)\n",
    "            outputs[:,t,:] = output\n",
    "\n",
    "            if torch.rand(1).item()< teacher_forcing_ratio:\n",
    "                input= trg[:,t,:].unsqueeze(1)\n",
    "            else:\n",
    "                input = output.unsqueeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=10, lr=1e-3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()  # For regression tasks (can be changed for classification tasks)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(src, trg)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example Data (Random for demonstration)\n",
    "src_data = torch.rand(100, 10, 5)  # (100 samples, 10 time steps, 5 features)\n",
    "trg_data = torch.rand(100, 10, 5)  # (100 samples, 10 time steps, 5 target features)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(src_data, trg_data)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.2669\n",
      "Epoch [2/200], Loss: 0.2201\n",
      "Epoch [3/200], Loss: 0.1606\n",
      "Epoch [4/200], Loss: 0.1203\n",
      "Epoch [5/200], Loss: 0.1125\n",
      "Epoch [6/200], Loss: 0.1090\n",
      "Epoch [7/200], Loss: 0.1095\n",
      "Epoch [8/200], Loss: 0.1069\n",
      "Epoch [9/200], Loss: 0.1057\n",
      "Epoch [10/200], Loss: 0.1066\n",
      "Epoch [11/200], Loss: 0.1067\n",
      "Epoch [12/200], Loss: 0.1074\n",
      "Epoch [13/200], Loss: 0.1058\n",
      "Epoch [14/200], Loss: 0.1068\n",
      "Epoch [15/200], Loss: 0.1059\n",
      "Epoch [16/200], Loss: 0.1065\n",
      "Epoch [17/200], Loss: 0.1076\n",
      "Epoch [18/200], Loss: 0.1066\n",
      "Epoch [19/200], Loss: 0.1090\n",
      "Epoch [20/200], Loss: 0.1071\n",
      "Epoch [21/200], Loss: 0.1060\n",
      "Epoch [22/200], Loss: 0.1068\n",
      "Epoch [23/200], Loss: 0.1060\n",
      "Epoch [24/200], Loss: 0.1058\n",
      "Epoch [25/200], Loss: 0.1071\n",
      "Epoch [26/200], Loss: 0.1073\n",
      "Epoch [27/200], Loss: 0.1066\n",
      "Epoch [28/200], Loss: 0.1078\n",
      "Epoch [29/200], Loss: 0.1058\n",
      "Epoch [30/200], Loss: 0.1060\n",
      "Epoch [31/200], Loss: 0.1055\n",
      "Epoch [32/200], Loss: 0.1067\n",
      "Epoch [33/200], Loss: 0.1062\n",
      "Epoch [34/200], Loss: 0.1066\n",
      "Epoch [35/200], Loss: 0.1050\n",
      "Epoch [36/200], Loss: 0.1075\n",
      "Epoch [37/200], Loss: 0.1062\n",
      "Epoch [38/200], Loss: 0.1056\n",
      "Epoch [39/200], Loss: 0.1064\n",
      "Epoch [40/200], Loss: 0.1049\n",
      "Epoch [41/200], Loss: 0.1055\n",
      "Epoch [42/200], Loss: 0.1068\n",
      "Epoch [43/200], Loss: 0.1060\n",
      "Epoch [44/200], Loss: 0.1050\n",
      "Epoch [45/200], Loss: 0.1059\n",
      "Epoch [46/200], Loss: 0.1078\n",
      "Epoch [47/200], Loss: 0.1062\n",
      "Epoch [48/200], Loss: 0.1067\n",
      "Epoch [49/200], Loss: 0.1069\n",
      "Epoch [50/200], Loss: 0.1050\n",
      "Epoch [51/200], Loss: 0.1055\n",
      "Epoch [52/200], Loss: 0.1069\n",
      "Epoch [53/200], Loss: 0.1071\n",
      "Epoch [54/200], Loss: 0.1063\n",
      "Epoch [55/200], Loss: 0.1057\n",
      "Epoch [56/200], Loss: 0.1070\n",
      "Epoch [57/200], Loss: 0.1063\n",
      "Epoch [58/200], Loss: 0.1059\n",
      "Epoch [59/200], Loss: 0.1065\n",
      "Epoch [60/200], Loss: 0.1086\n",
      "Epoch [61/200], Loss: 0.1063\n",
      "Epoch [62/200], Loss: 0.1071\n",
      "Epoch [63/200], Loss: 0.1064\n",
      "Epoch [64/200], Loss: 0.1064\n",
      "Epoch [65/200], Loss: 0.1061\n",
      "Epoch [66/200], Loss: 0.1076\n",
      "Epoch [67/200], Loss: 0.1058\n",
      "Epoch [68/200], Loss: 0.1068\n",
      "Epoch [69/200], Loss: 0.1040\n",
      "Epoch [70/200], Loss: 0.1071\n",
      "Epoch [71/200], Loss: 0.1048\n",
      "Epoch [72/200], Loss: 0.1060\n",
      "Epoch [73/200], Loss: 0.1072\n",
      "Epoch [74/200], Loss: 0.1050\n",
      "Epoch [75/200], Loss: 0.1063\n",
      "Epoch [76/200], Loss: 0.1074\n",
      "Epoch [77/200], Loss: 0.1072\n",
      "Epoch [78/200], Loss: 0.1076\n",
      "Epoch [79/200], Loss: 0.1060\n",
      "Epoch [80/200], Loss: 0.1052\n",
      "Epoch [81/200], Loss: 0.1065\n",
      "Epoch [82/200], Loss: 0.1051\n",
      "Epoch [83/200], Loss: 0.1071\n",
      "Epoch [84/200], Loss: 0.1065\n",
      "Epoch [85/200], Loss: 0.1060\n",
      "Epoch [86/200], Loss: 0.1069\n",
      "Epoch [87/200], Loss: 0.1068\n",
      "Epoch [88/200], Loss: 0.1083\n",
      "Epoch [89/200], Loss: 0.1056\n",
      "Epoch [90/200], Loss: 0.1076\n",
      "Epoch [91/200], Loss: 0.1047\n",
      "Epoch [92/200], Loss: 0.1059\n",
      "Epoch [93/200], Loss: 0.1055\n",
      "Epoch [94/200], Loss: 0.1054\n",
      "Epoch [95/200], Loss: 0.1065\n",
      "Epoch [96/200], Loss: 0.1045\n",
      "Epoch [97/200], Loss: 0.1061\n",
      "Epoch [98/200], Loss: 0.1061\n",
      "Epoch [99/200], Loss: 0.1073\n",
      "Epoch [100/200], Loss: 0.1064\n",
      "Epoch [101/200], Loss: 0.1072\n",
      "Epoch [102/200], Loss: 0.1063\n",
      "Epoch [103/200], Loss: 0.1062\n",
      "Epoch [104/200], Loss: 0.1064\n",
      "Epoch [105/200], Loss: 0.1074\n",
      "Epoch [106/200], Loss: 0.1061\n",
      "Epoch [107/200], Loss: 0.1062\n",
      "Epoch [108/200], Loss: 0.1067\n",
      "Epoch [109/200], Loss: 0.1061\n",
      "Epoch [110/200], Loss: 0.1050\n",
      "Epoch [111/200], Loss: 0.1064\n",
      "Epoch [112/200], Loss: 0.1060\n",
      "Epoch [113/200], Loss: 0.1066\n",
      "Epoch [114/200], Loss: 0.1065\n",
      "Epoch [115/200], Loss: 0.1071\n",
      "Epoch [116/200], Loss: 0.1068\n",
      "Epoch [117/200], Loss: 0.1065\n",
      "Epoch [118/200], Loss: 0.1059\n",
      "Epoch [119/200], Loss: 0.1063\n",
      "Epoch [120/200], Loss: 0.1061\n",
      "Epoch [121/200], Loss: 0.1052\n",
      "Epoch [122/200], Loss: 0.1069\n",
      "Epoch [123/200], Loss: 0.1074\n",
      "Epoch [124/200], Loss: 0.1040\n",
      "Epoch [125/200], Loss: 0.1067\n",
      "Epoch [126/200], Loss: 0.1065\n",
      "Epoch [127/200], Loss: 0.1045\n",
      "Epoch [128/200], Loss: 0.1061\n",
      "Epoch [129/200], Loss: 0.1049\n",
      "Epoch [130/200], Loss: 0.1070\n",
      "Epoch [131/200], Loss: 0.1056\n",
      "Epoch [132/200], Loss: 0.1075\n",
      "Epoch [133/200], Loss: 0.1050\n",
      "Epoch [134/200], Loss: 0.1073\n",
      "Epoch [135/200], Loss: 0.1061\n",
      "Epoch [136/200], Loss: 0.1067\n",
      "Epoch [137/200], Loss: 0.1055\n",
      "Epoch [138/200], Loss: 0.1056\n",
      "Epoch [139/200], Loss: 0.1058\n",
      "Epoch [140/200], Loss: 0.1059\n",
      "Epoch [141/200], Loss: 0.1050\n",
      "Epoch [142/200], Loss: 0.1043\n",
      "Epoch [143/200], Loss: 0.1063\n",
      "Epoch [144/200], Loss: 0.1066\n",
      "Epoch [145/200], Loss: 0.1064\n",
      "Epoch [146/200], Loss: 0.1060\n",
      "Epoch [147/200], Loss: 0.1054\n",
      "Epoch [148/200], Loss: 0.1046\n",
      "Epoch [149/200], Loss: 0.1046\n",
      "Epoch [150/200], Loss: 0.1069\n",
      "Epoch [151/200], Loss: 0.1072\n",
      "Epoch [152/200], Loss: 0.1054\n",
      "Epoch [153/200], Loss: 0.1070\n",
      "Epoch [154/200], Loss: 0.1064\n",
      "Epoch [155/200], Loss: 0.1043\n",
      "Epoch [156/200], Loss: 0.1043\n",
      "Epoch [157/200], Loss: 0.1046\n",
      "Epoch [158/200], Loss: 0.1063\n",
      "Epoch [159/200], Loss: 0.1051\n",
      "Epoch [160/200], Loss: 0.1067\n",
      "Epoch [161/200], Loss: 0.1043\n",
      "Epoch [162/200], Loss: 0.1075\n",
      "Epoch [163/200], Loss: 0.1065\n",
      "Epoch [164/200], Loss: 0.1068\n",
      "Epoch [165/200], Loss: 0.1064\n",
      "Epoch [166/200], Loss: 0.1051\n",
      "Epoch [167/200], Loss: 0.1070\n",
      "Epoch [168/200], Loss: 0.1052\n",
      "Epoch [169/200], Loss: 0.1054\n",
      "Epoch [170/200], Loss: 0.1056\n",
      "Epoch [171/200], Loss: 0.1052\n",
      "Epoch [172/200], Loss: 0.1063\n",
      "Epoch [173/200], Loss: 0.1051\n",
      "Epoch [174/200], Loss: 0.1052\n",
      "Epoch [175/200], Loss: 0.1081\n",
      "Epoch [176/200], Loss: 0.1063\n",
      "Epoch [177/200], Loss: 0.1057\n",
      "Epoch [178/200], Loss: 0.1047\n",
      "Epoch [179/200], Loss: 0.1060\n",
      "Epoch [180/200], Loss: 0.1049\n",
      "Epoch [181/200], Loss: 0.1049\n",
      "Epoch [182/200], Loss: 0.1058\n",
      "Epoch [183/200], Loss: 0.1055\n",
      "Epoch [184/200], Loss: 0.1035\n",
      "Epoch [185/200], Loss: 0.1044\n",
      "Epoch [186/200], Loss: 0.1039\n",
      "Epoch [187/200], Loss: 0.1042\n",
      "Epoch [188/200], Loss: 0.1051\n",
      "Epoch [189/200], Loss: 0.1036\n",
      "Epoch [190/200], Loss: 0.1063\n",
      "Epoch [191/200], Loss: 0.1054\n",
      "Epoch [192/200], Loss: 0.1050\n",
      "Epoch [193/200], Loss: 0.1047\n",
      "Epoch [194/200], Loss: 0.1046\n",
      "Epoch [195/200], Loss: 0.1044\n",
      "Epoch [196/200], Loss: 0.1045\n",
      "Epoch [197/200], Loss: 0.1025\n",
      "Epoch [198/200], Loss: 0.1042\n",
      "Epoch [199/200], Loss: 0.1042\n",
      "Epoch [200/200], Loss: 0.1037\n"
     ]
    }
   ],
   "source": [
    "#Initialize Model\n",
    "input_dim = 5\n",
    "output_dim = 5\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "model = Seq2Seq(input_dim, output_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Train the Model\n",
    "train(model, train_loader, epochs=200, lr=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
